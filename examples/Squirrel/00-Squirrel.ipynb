{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as ss\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.pre_processing import Vulture\n",
    "from TELF.pre_processing.Vulture.modules import SimpleCleaner\n",
    "from TELF.pre_processing.Vulture.modules import LemmatizeCleaner\n",
    "from TELF.pre_processing.Vulture.modules import RemoveNonEnglishCleaner\n",
    "from TELF.pre_processing.Vulture.default_stop_words import STOP_WORDS\n",
    "from TELF.pre_processing.Vulture.default_stop_phrases import STOP_PHRASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.factorization.HNMFk import HNMFk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.pre_processing import Beaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.post_processing import ArcticFox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.helpers.file_system import find_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.pre_processing.Squirrel import Squirrel\n",
    "from TELF.pre_processing.Squirrel.pruners import EmbeddingPruner\n",
    "from TELF.pre_processing.Squirrel.pruners import LLMPruner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"..\", \"..\", \"data\", \"sample2.csv\"))\n",
    "df = df.head(50).reset_index(drop=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    RemoveNonEnglishCleaner(ascii_ratio=0.9, stopwords_ratio=0.25),\n",
    "    SimpleCleaner(stop_words = STOP_WORDS,\n",
    "                  stop_phrases = STOP_PHRASES,\n",
    "                  order = [\n",
    "                      'standardize_hyphens',\n",
    "                      'isolate_frozen',\n",
    "                      'remove_copyright_statement',\n",
    "                      'remove_stop_phrases',\n",
    "                      'make_lower_case',\n",
    "                      'remove_formulas',\n",
    "                      'normalize',\n",
    "                      'remove_next_line',\n",
    "                      'remove_email',\n",
    "                      'remove_()',\n",
    "                      'remove_[]',\n",
    "                      'remove_special_characters',\n",
    "                      'remove_nonASCII_boundary',\n",
    "                      'remove_nonASCII',\n",
    "                      'remove_tags',\n",
    "                      'remove_stop_words',\n",
    "                      'remove_standalone_numbers',\n",
    "                      'remove_extra_whitespace',\n",
    "                      'min_characters',\n",
    "                  ]\n",
    "                 ),\n",
    "    LemmatizeCleaner('spacy'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vulture = Vulture(n_jobs=1, verbose=10)\n",
    "df = vulture.clean_dataframe(df=df, \n",
    "                        columns=[\"abstract\", \"title\"],\n",
    "                        append_to_original_df=True,\n",
    "                        concat_cleaned_cols=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.clean_abstract_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build The Vocabulary and the Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'clean_abstract_title'\n",
    "RESULTS = \"result_example\"\n",
    "HIGHLIGHT_WORDS = ['analysis', 'tensor']\n",
    "HIGHLIGHT_WEIGHTS = [2 for i in HIGHLIGHT_WORDS]\n",
    "beaver = Beaver()\n",
    "os.makedirs(RESULTS, exist_ok=True)\n",
    "settings = {\n",
    "    \"dataset\" : df,\n",
    "    \"target_column\" : DATA_COLUMN,\n",
    "    'highlighting': HIGHLIGHT_WORDS,\n",
    "    'weights':HIGHLIGHT_WEIGHTS,\n",
    "    \"matrix_type\" : \"tfidf\",\n",
    "    \"save_path\" : RESULTS\n",
    "}\n",
    "X, vocabulary = beaver.documents_words(**settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.T.tocsr()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape[1] == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorize with HNMFk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of cluster numbers (K) to search over\n",
    "Ks = np.arange(2, 10, 1)  # From 2 to 29 inclusive\n",
    "\n",
    "# Number of perturbations and iterations to run\n",
    "perts = 2  # Number of perturbed runs to estimate stability\n",
    "iters = 2  # Number of iterations for each perturbation\n",
    "\n",
    "# Small perturbation epsilon added to input data\n",
    "eps = 0.025\n",
    "\n",
    "# Initialization method for NMF (Non-negative Matrix Factorization)\n",
    "init = \"nnsvd\"  # Nonnegative SVD initialization\n",
    "\n",
    "# Path to save HNMFk results\n",
    "HNMFK_save_path = os.path.join(RESULTS, \"example_HNMFK\")\n",
    "name = HNMFK_save_path  # Alias for convenience\n",
    "\n",
    "# Parameters for HNMFk (Hierarchical Nonnegative Matrix Factorization k-search)\n",
    "nmfk_params = {\n",
    "    \"k_search_method\": \"bst_pre\",             # Method for determining optimal k (e.g., binary search with pre-checks)\n",
    "    \"sill_thresh\": 0.7,                       # Silhouette threshold to accept a given k\n",
    "    \"H_sill_thresh\": 0.05,                    # Threshold for H-matrix silhouette to refine k selection\n",
    "    \"n_perturbs\": perts,                      # Number of perturbations\n",
    "    \"n_iters\": iters,                         # Number of iterations per perturbation\n",
    "    \"epsilon\": eps,                           # Perturbation strength\n",
    "    \"n_jobs\": -1,                             # Use all available CPU cores\n",
    "    \"init\": init,                             # NMF initialization method\n",
    "    \"use_gpu\": False,                         # Whether to use GPU acceleration\n",
    "    \"save_path\": HNMFK_save_path,             # Directory where results will be saved\n",
    "    \"predict_k_method\": \"WH_sill\",            # Method to predict k using W and H matrix silhouettes\n",
    "    \"predict_k\": True,                        # Whether to automatically predict k\n",
    "    \"verbose\": False,                          # Verbose output\n",
    "    \"nmf_verbose\": False,                     # Verbose output from NMF algorithm\n",
    "    \"transpose\": False,                       # Whether to transpose input data\n",
    "    \"pruned\": True,                           # Whether to prune unstable clusters\n",
    "    \"nmf_method\": \"nmf_fro_mu\",               # NMF solver method (Frobenius norm, multiplicative updates)\n",
    "    \"calculate_error\": False,                 # Whether to calculate reconstruction error\n",
    "    \"use_consensus_stopping\": 0,              # Whether to use consensus stopping (0 = off)\n",
    "    \"calculate_pac\": False,                   # Whether to compute PAC (proportion of ambiguous clustering)\n",
    "    \"consensus_mat\": False,                   # Whether to generate consensus matrix\n",
    "    \"perturb_type\": \"uniform\",                # Type of perturbation (e.g., uniform noise)\n",
    "    \"perturb_multiprocessing\": False,         # Use multiprocessing during perturbation\n",
    "    \"perturb_verbose\": False,                 # Verbose output during perturbation\n",
    "    \"simple_plot\": True                       # Whether to generate simplified summary plots\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSemanticCallback:\n",
    "    def __init__(self, \n",
    "                 df: pd.DataFrame, \n",
    "                 target_column=DATA_COLUMN,\n",
    "                 options={'vocabulary': vocabulary},\n",
    "                 matrix_type=\"tfidf\") -> None:\n",
    "        \"\"\"\n",
    "        Initializes the callback with a DataFrame and matrix generation settings.\n",
    "\n",
    "        Parameters:\n",
    "        - df: The full DataFrame containing the text data.\n",
    "        - target_column: Column name containing the target text to vectorize (default is a global DATA_COLUMN).\n",
    "        - options: Options dictionary passed to Beaver (e.g., fixed vocabulary, token settings).\n",
    "        - matrix_type: Type of vectorization matrix (e.g., \"tfidf\", \"count\").\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.target_column = target_column\n",
    "        self.options = options\n",
    "        self.matrix_type = matrix_type\n",
    "\n",
    "    def __call__(self, original_indices: np.ndarray):\n",
    "        \"\"\"\n",
    "        Callable interface for dynamically generating document-term matrices \n",
    "        from a subset of the DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        - original_indices: Numpy array of row indices from self.df to subset and transform.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple of (X, metadata), where:\n",
    "            - X is a document-term sparse matrix (CSR format).\n",
    "            - metadata is a dict containing either 'vocab' or a 'stop_reason' if failed.\n",
    "        \"\"\"\n",
    "        current_beaver = Beaver()  # Initialize a new instance of the Beaver text vectorizer\n",
    "\n",
    "        # Extract the subset of the DataFrame using the provided indices\n",
    "        current_df = self.df.iloc[original_indices].copy()\n",
    "\n",
    "        # Construct parameters for the Beaver vectorizer\n",
    "        current_beaver_matrix_settings = {\n",
    "            \"dataset\": current_df,\n",
    "            \"target_column\": self.target_column,\n",
    "            \"options\": self.options,\n",
    "            \"highlighting\": HIGHLIGHT_WORDS,     # Global list of words to highlight\n",
    "            \"weights\": HIGHLIGHT_WEIGHTS,        # Associated weights for highlighting\n",
    "            \"matrix_type\": self.matrix_type,     # Type of matrix to construct (e.g., TF-IDF)\n",
    "            \"save_path\": None                    # No file output; matrix is returned\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Attempt to generate the document-word matrix\n",
    "            current_X, vocab = current_beaver.documents_words(**current_beaver_matrix_settings)\n",
    "            \n",
    "            # Transpose to get documents as rows (CSR format is efficient for row slicing)\n",
    "            current_X = current_X.T.tocsr()\n",
    "            \n",
    "            return current_X, {'vocab': vocab}\n",
    "\n",
    "        except:\n",
    "            # On failure, return a 1x1 matrix to signal a stopping condition for downstream tasks\n",
    "            csr_matrix = ss.csr_matrix([[1]])\n",
    "            return csr_matrix, {'stop_reason': \"documents_words couldn't make matrix\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for initializing and training the HNMFk model\n",
    "hnmfk_params = {\n",
    "    \"n_nodes\": 1,  # Number of root nodes to begin with (can grow as depth increases)\n",
    "    \n",
    "    # List of NMF parameters for the top-level (depth=0); can use different sets for different nodes\n",
    "    \"nmfk_params\": [nmfk_params],  \n",
    "    \n",
    "    # Callable that generates a document-term matrix from a subset of the DataFrame (dynamic input for each node)\n",
    "    \"generate_X_callback\": CustomSemanticCallback(df=df, options={'vocabulary': vocabulary}),\n",
    "    \n",
    "    \"cluster_on\": \"H\",  # Which factor matrix to use for clustering (H = document-topic)\n",
    "    \n",
    "    \"depth\": 1,  # Depth of the hierarchy; e.g., 2 means root + one layer of children\n",
    "    \n",
    "    \"sample_thresh\": 10,  # Minimum number of samples required to split/cluster a node further\n",
    "    \n",
    "    \"K2\": False,  # If True, forces all subclusters to use k=2; here we allow varying k\n",
    "    \n",
    "    # Range of K to try for deeper layers (children nodes)\n",
    "    \"Ks_deep_min\": 1,\n",
    "    \"Ks_deep_max\": 20,\n",
    "    \"Ks_deep_step\": 1,\n",
    "    \n",
    "    \"experiment_name\": name,  # Folder/identifier for saving results and checkpoints\n",
    "}\n",
    "\n",
    "# Instantiate the HNMFk model with the above parameters\n",
    "model = HNMFk(**hnmfk_params)\n",
    "\n",
    "# Fit the model on matrix X using the specified range of Ks\n",
    "# - from_checkpoint: load previously saved progress if available\n",
    "# - save_checkpoint: periodically save progress for recovery or inspection\n",
    "model.fit(X, Ks, from_checkpoint=False, save_checkpoint=True)\n",
    "\n",
    "# Traverse and collect all nodes created in the hierarchical model\n",
    "all_nodes = model.traverse_nodes()\n",
    "print(len(all_nodes))  # Output the total number of nodes (clusters at all levels)\n",
    "\n",
    "# Save the full trained model to a pickle file for reuse or inspection\n",
    "with open(os.path.join('result_example', 'HNMFK_highlight.pkl'), 'wb') as output_file:\n",
    "    pickle.dump(model, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained HNMFk model from disk\n",
    "model = HNMFk(experiment_name=os.path.join(\"result_example\", \"example_HNMFK\"))\n",
    "model.load_model()  # Loads model from the provided experiment_name path\n",
    "\n",
    "# Initialize ArcticFox pipeline\n",
    "# - model: the hierarchical clustering model (HNMFk)\n",
    "# - embedding_model: name of the sentence embedding model used for label generation\n",
    "# - clean_cols_name: column in the DataFrame containing the cleaned text input\n",
    "pipeline = ArcticFox(\n",
    "    model=model,\n",
    "    embedding_model=\"SCINCL\",        # Example: SCINCL embedding model fine-tuned for scientific text\n",
    "    clean_cols_name=DATA_COLUMN      # The text column used for label generation and analysis\n",
    ")\n",
    "\n",
    "# This handles hierarchical processing\n",
    "pipeline.run_full_pipeline(\n",
    "    vocab=vocabulary,                # Vocabulary used to guide or filter cluster content\n",
    "    data_df=df,                      # Original dataset (same used in HNMFk)\n",
    "    label_clusters=False,             # Enable automatic labeling of clusters\n",
    "    generate_stats=False,             # Generate cluster-level statistics\n",
    "    process_parents=True,            # Propagate labels or stats upward through the hierarchy\n",
    "    skip_completed=True,             # Skip processing of nodes already labeled/stored\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune with Squirrel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 23 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   eid                             50 non-null     object \n",
      " 1   s2id                            50 non-null     object \n",
      " 2   doi                             50 non-null     object \n",
      " 3   title                           50 non-null     object \n",
      " 4   abstract                        50 non-null     object \n",
      " 5   year                            50 non-null     int64  \n",
      " 6   authors                         50 non-null     object \n",
      " 7   author_ids                      50 non-null     object \n",
      " 8   affiliations                    50 non-null     object \n",
      " 9   funding                         5 non-null      object \n",
      " 10  PACs                            8 non-null      object \n",
      " 11  publication_name                50 non-null     object \n",
      " 12  subject_areas                   50 non-null     object \n",
      " 13  s2_authors                      50 non-null     object \n",
      " 14  s2_author_ids                   50 non-null     object \n",
      " 15  citations                       45 non-null     object \n",
      " 16  references                      38 non-null     object \n",
      " 17  num_citations                   50 non-null     int64  \n",
      " 18  num_references                  50 non-null     float64\n",
      " 19  clean_abstract_title            50 non-null     object \n",
      " 20  cluster                         50 non-null     int64  \n",
      " 21  cluster_coordinates             50 non-null     object \n",
      " 22  similarity_to_cluster_centroid  50 non-null     float64\n",
      "dtypes: float64(2), int64(3), object(18)\n",
      "memory usage: 9.1+ KB\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join('result_example', 'example_HNMFK', 'depth_0', 'Root')\n",
    "start_with = 'cluster_for_k='\n",
    "\n",
    "csv_data_path = find_files(path =path, start_with=start_with)[0]\n",
    "\n",
    "df = pd.read_csv(csv_data_path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'llama3.2:latest' not found locally – pulling…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c473748d0e242a580738f1940ef0d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LLM voting:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/barron/miniconda3/envs/dev_artic_fox/lib/python3.11/site-packages/TELF/pre_processing/Squirrel/pruners/llm_prune.py:83: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.at[idx, self.NAME] = decision\n",
      "Bad JSON from LLM: Yes. The candidate abstract appears to be related to the concept of SeNMFk (Semi-Negative Matrix Factorization with automatic model selection), which was mentioned in the original text as a method for topic modeling and extracting sub-topics from large datasets, including scientific literature.\n",
      "Bad JSON from LLM: Yes.\n",
      "\n",
      "The candidate abstract is related to one of the concepts mentioned:\n",
      "\n",
      "1. **Topic Modeling**: The abstract discusses a hierarchical SeNMFk approach for topic modeling in scientific literature, which is a relevant concept discussed earlier in the provided text.\n",
      "2. **Malware Detection**: While not directly related to topic modeling, the abstract mentions malware detection and its connection to the proposed HNMFk Classifier, which suggests that the candidate abstract is also related to a specific application of machine learning techniques.\n",
      "\n",
      "However, it's worth noting that the exact details of the proposed tool for generating targeted datasets of scientific literature are not discussed in the candidate abstract.\n",
      "Bad JSON from LLM: {\"answer\":\"yes\",\"reason\":\"The candidate abstract mentions \"HNMFk Classifier\", \"SeNMFk\", and \"topic modeling\" which are all related to the concepts discussed in the original text. Specifically, it appears to be a continuation of the research on semi-negative matrix factorization (SeNMF) and its application to topic modeling and hierarchical clustering of text data.\"}\n",
      "Bad JSON from LLM: Yes. The candidate is about machine learning and deep learning algorithms, specifically about non-negative matrix factorization (NMF), truncated singular value decomposition (t-SVD), hierarchical semi-supervised classification, anomaly detection in cybersecurity, and distributed computing for high-performance computing (HPC) systems. \n",
      "\n",
      "More specifically, it covers topics such as:\n",
      "\n",
      "1. Efficient distributed implementation of NMF for large-scale data.\n",
      "2. Truncated SVD algorithm with a power method-based approach for efficient computation.\n",
      "3. Hierarchical semi-supervised classification of malware families using non-negative matrix factorization.\n",
      "\n",
      "The candidate is likely related to the field of machine learning and deep learning, particularly in areas like computer vision, natural language processing, and cybersecurity.\n",
      "Bad JSON from LLM: Yes. The text appears to be related to machine learning and data analysis, specifically discussing novel algorithms for malware classification, non-negative matrix factorization (NMF), and truncated singular value decomposition (t-SVD) for high-performance computing systems.\n",
      "Bad JSON from LLM: Yes. \n",
      "\n",
      "The paper appears to be related to machine learning and data analysis, specifically in the areas of malware detection, anomaly detection, and singular value decomposition (SVD) optimization for high-performance computing systems. The text mentions concepts such as hierarchical semi-supervised classification, non-negative matrix factorization, and distributed out-of-memory algorithms for NMF and SVD.\n",
      "Bad JSON from LLM: Yes. \n",
      "\n",
      "The text appears to be discussing various machine learning and computer science topics related to malware classification, non-negative matrix factorization (NMF), hierarchical semi-supervised algorithms, distributed out-of-memory implementations of NMF and Singular Value Decomposition (SVD) for high-performance computing systems, and neural networks.\n",
      "Bad JSON from LLM: The candidate does not seem to be directly related to any specific concept mentioned in the provided text. The topics covered include:\n",
      "\n",
      "1. Optimizing hyperparameters in large language models AutoML.\n",
      "2. Dimensionality reduction techniques for big data.\n",
      "3. Malware detection and classification.\n",
      "4. Efficient implementation of truncated singular value decomposition (t-SVD) for heterogeneous high-performance computing systems.\n",
      "5. Supervised learning for malware/benign-ware classification.\n",
      "6. Support vector machines for high-dimensional spaces.\n",
      "7. Hyperparameter tuning for machine learning models.\n",
      "8. Overfitting in machine learning models.\n",
      "9. Deep learning and computer vision/natural language processing.\n",
      "10. Supervisory Control and Data Acquisition (SCADA) systems.\n",
      "\n",
      "However, the candidate does mention tensor decomposition, which is related to one of these topics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 13 entries, 7 to 47\n",
      "Data columns (total 26 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   eid                             13 non-null     object \n",
      " 1   s2id                            13 non-null     object \n",
      " 2   doi                             13 non-null     object \n",
      " 3   title                           13 non-null     object \n",
      " 4   abstract                        13 non-null     object \n",
      " 5   year                            13 non-null     int64  \n",
      " 6   authors                         13 non-null     object \n",
      " 7   author_ids                      13 non-null     object \n",
      " 8   affiliations                    13 non-null     object \n",
      " 9   funding                         1 non-null      object \n",
      " 10  PACs                            2 non-null      object \n",
      " 11  publication_name                13 non-null     object \n",
      " 12  subject_areas                   13 non-null     object \n",
      " 13  s2_authors                      13 non-null     object \n",
      " 14  s2_author_ids                   13 non-null     object \n",
      " 15  citations                       12 non-null     object \n",
      " 16  references                      9 non-null      object \n",
      " 17  num_citations                   13 non-null     int64  \n",
      " 18  num_references                  13 non-null     float64\n",
      " 19  clean_abstract_title            13 non-null     object \n",
      " 20  cluster                         13 non-null     int64  \n",
      " 21  cluster_coordinates             13 non-null     object \n",
      " 22  similarity_to_cluster_centroid  13 non-null     float64\n",
      " 23  title_abstract                  13 non-null     object \n",
      " 24  embed_prune                     13 non-null     bool   \n",
      " 25  llm_prune                       13 non-null     object \n",
      "dtypes: bool(1), float64(2), int64(3), object(20)\n",
      "memory usage: 2.7+ KB\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PRUNING_DIR = Path(\"example_output\")\n",
    "LABEL_COLUMN      = \"cluster\"\n",
    "LABEL_VALUE       = 7\n",
    "df['title_abstract'] = df['title'] + \" \" + df['abstract']\n",
    "\n",
    "emb_pruner = EmbeddingPruner(\n",
    "    embedding_model=\"SCINCL\",\n",
    "    distance_std_factor=3.0,\n",
    "    overwrite_embeddings=False,\n",
    "    use_gpu=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "llm_pruner = LLMPruner(\n",
    "    llm_model_name=\"llama3.2:latest\",\n",
    "    llm_api_url=\"http://localhost:11434\",\n",
    "    llm_vote_trials=4,\n",
    "    llm_promote_threshold=0.75,\n",
    "    llm_temperature=0.7,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "pipeline = [emb_pruner, llm_pruner]\n",
    "\n",
    "processor = Squirrel(\n",
    "    data_source=df,\n",
    "    output_dir=OUTPUT_PRUNING_DIR,\n",
    "    label_column=LABEL_COLUMN,\n",
    "    reference_label=LABEL_VALUE,\n",
    "    pipeline=pipeline\n",
    ")\n",
    "\n",
    "final_df = processor()\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eid</th>\n",
       "      <th>s2id</th>\n",
       "      <th>doi</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "      <th>authors</th>\n",
       "      <th>author_ids</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>funding</th>\n",
       "      <th>...</th>\n",
       "      <th>references</th>\n",
       "      <th>num_citations</th>\n",
       "      <th>num_references</th>\n",
       "      <th>clean_abstract_title</th>\n",
       "      <th>cluster</th>\n",
       "      <th>cluster_coordinates</th>\n",
       "      <th>similarity_to_cluster_centroid</th>\n",
       "      <th>title_abstract</th>\n",
       "      <th>embed_prune</th>\n",
       "      <th>llm_prune</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0998b26c-0630-4af8-9967-7c59455a3460</td>\n",
       "      <td>35f3648e-7938-4d1a-88fd-606cc6684e0c</td>\n",
       "      <td>dc6762f8-270a-44b2-8a37-27687969b246</td>\n",
       "      <td>Using GANs for Realistic Image Synthesis Graph...</td>\n",
       "      <td>Data preprocessing steps such as normalization...</td>\n",
       "      <td>1996</td>\n",
       "      <td>Rasmussen K.</td>\n",
       "      <td>0506d5ab-b679-415f-a3ce-40c762a73251</td>\n",
       "      <td>{'f05e4224-11eb-4c77-b9be-917a55eeb9b1': {'cou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>51a23b45-b324-409a-85db-22bb9d5e25e5;51a23b45-...</td>\n",
       "      <td>38</td>\n",
       "      <td>9.0</td>\n",
       "      <td>preprocessing steps normalization handling mis...</td>\n",
       "      <td>4</td>\n",
       "      <td>[1.1920929e-07, 1.1920929e-07, 1.1920929e-07, ...</td>\n",
       "      <td>1.336318</td>\n",
       "      <td>Using GANs for Realistic Image Synthesis Graph...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>081f8efb-d812-4b7e-93e9-2ce40e46528a</td>\n",
       "      <td>5c8bf977-37b5-46b5-b430-76967d41ee41</td>\n",
       "      <td>6512799b-99f5-4967-bd6f-27b362ecc8e9</td>\n",
       "      <td>The Intersection of AI and IoT in Smart Cities</td>\n",
       "      <td>Feature engineering plays a crucial role in im...</td>\n",
       "      <td>1996</td>\n",
       "      <td>Rasmussen K.</td>\n",
       "      <td>0506d5ab-b679-415f-a3ce-40c762a73251</td>\n",
       "      <td>{'fcb9d5e2-a57f-45fa-b650-4a1a1e3d11fd': {'cou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>feature engineering plays crucial role improvi...</td>\n",
       "      <td>7</td>\n",
       "      <td>[1.1920929e-07, 1.1920929e-07, 1.1920929e-07, ...</td>\n",
       "      <td>1.043619</td>\n",
       "      <td>The Intersection of AI and IoT in Smart Cities...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>06612d14-fefc-4185-bc72-5cc755dced6f</td>\n",
       "      <td>b5e2a4c5-a2e0-40ef-b13b-3b0dc33fc7ca</td>\n",
       "      <td>04726b3b-2014-48e9-9c11-ceaf704c1ffc</td>\n",
       "      <td>Neural Architecture Search for Optimized Model...</td>\n",
       "      <td>Cybersecurity frameworks like NIST provide gui...</td>\n",
       "      <td>2002</td>\n",
       "      <td>Rasmussen K.</td>\n",
       "      <td>0506d5ab-b679-415f-a3ce-40c762a73251</td>\n",
       "      <td>{'d1361bd1-ee0f-4550-99f9-4b6939fccfcd': {'cou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>f49acf4d-1287-499e-bb1f-a571a7b4caa7;e09fc743-...</td>\n",
       "      <td>87</td>\n",
       "      <td>70.0</td>\n",
       "      <td>cybersecurity frameworks nist guidelines risk ...</td>\n",
       "      <td>7</td>\n",
       "      <td>[1.1920929e-07, 0.30378982, 1.1920929e-07, 1.1...</td>\n",
       "      <td>1.013045</td>\n",
       "      <td>Neural Architecture Search for Optimized Model...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8bd1dc3d-421a-441d-9fa4-21e624d24ced</td>\n",
       "      <td>e09fc743-d65b-4de1-b123-0d5c2b4cdc73</td>\n",
       "      <td>1ce36fd1-e394-4e5d-bf45-567ae88b3b8c</td>\n",
       "      <td>Unraveling the Mystery of Black Box AI</td>\n",
       "      <td>Identification of the family to which a malwar...</td>\n",
       "      <td>2001</td>\n",
       "      <td>Rasmussen K.</td>\n",
       "      <td>0506d5ab-b679-415f-a3ce-40c762a73251</td>\n",
       "      <td>{'f05e4224-11eb-4c77-b9be-917a55eeb9b1': {'cou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>identification family malware specimen belongs...</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.43752947, 1.1920929e-07, 1.1920929e-07, 1.1...</td>\n",
       "      <td>0.854075</td>\n",
       "      <td>Unraveling the Mystery of Black Box AI Identif...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4df6a9cd-c60f-4d46-a30d-d7c9b0bf0bea</td>\n",
       "      <td>0f4c886a-33c4-483f-ad03-8274eecd9138</td>\n",
       "      <td>d984689d-9e65-43ae-a60e-a22f31c39358</td>\n",
       "      <td>Unraveling the Mystery of Black Box AI Explori...</td>\n",
       "      <td>Malware is one of the most dangerous and costl...</td>\n",
       "      <td>1997</td>\n",
       "      <td>Rasmussen K.</td>\n",
       "      <td>0506d5ab-b679-415f-a3ce-40c762a73251</td>\n",
       "      <td>{'f05e4224-11eb-4c77-b9be-917a55eeb9b1': {'cou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ad371947-f480-41cf-8e2b-256548e91e6b;51a23b45-...</td>\n",
       "      <td>90</td>\n",
       "      <td>4.0</td>\n",
       "      <td>malware dangerous costly cyber threats nationa...</td>\n",
       "      <td>8</td>\n",
       "      <td>[0.98035914, 1.1920929e-07, 1.1920929e-07, 0.2...</td>\n",
       "      <td>1.166755</td>\n",
       "      <td>Unraveling the Mystery of Black Box AI Explori...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     eid  \\\n",
       "7   0998b26c-0630-4af8-9967-7c59455a3460   \n",
       "8   081f8efb-d812-4b7e-93e9-2ce40e46528a   \n",
       "12  06612d14-fefc-4185-bc72-5cc755dced6f   \n",
       "15  8bd1dc3d-421a-441d-9fa4-21e624d24ced   \n",
       "21  4df6a9cd-c60f-4d46-a30d-d7c9b0bf0bea   \n",
       "\n",
       "                                    s2id  \\\n",
       "7   35f3648e-7938-4d1a-88fd-606cc6684e0c   \n",
       "8   5c8bf977-37b5-46b5-b430-76967d41ee41   \n",
       "12  b5e2a4c5-a2e0-40ef-b13b-3b0dc33fc7ca   \n",
       "15  e09fc743-d65b-4de1-b123-0d5c2b4cdc73   \n",
       "21  0f4c886a-33c4-483f-ad03-8274eecd9138   \n",
       "\n",
       "                                     doi  \\\n",
       "7   dc6762f8-270a-44b2-8a37-27687969b246   \n",
       "8   6512799b-99f5-4967-bd6f-27b362ecc8e9   \n",
       "12  04726b3b-2014-48e9-9c11-ceaf704c1ffc   \n",
       "15  1ce36fd1-e394-4e5d-bf45-567ae88b3b8c   \n",
       "21  d984689d-9e65-43ae-a60e-a22f31c39358   \n",
       "\n",
       "                                                title  \\\n",
       "7   Using GANs for Realistic Image Synthesis Graph...   \n",
       "8      The Intersection of AI and IoT in Smart Cities   \n",
       "12  Neural Architecture Search for Optimized Model...   \n",
       "15             Unraveling the Mystery of Black Box AI   \n",
       "21  Unraveling the Mystery of Black Box AI Explori...   \n",
       "\n",
       "                                             abstract  year       authors  \\\n",
       "7   Data preprocessing steps such as normalization...  1996  Rasmussen K.   \n",
       "8   Feature engineering plays a crucial role in im...  1996  Rasmussen K.   \n",
       "12  Cybersecurity frameworks like NIST provide gui...  2002  Rasmussen K.   \n",
       "15  Identification of the family to which a malwar...  2001  Rasmussen K.   \n",
       "21  Malware is one of the most dangerous and costl...  1997  Rasmussen K.   \n",
       "\n",
       "                              author_ids  \\\n",
       "7   0506d5ab-b679-415f-a3ce-40c762a73251   \n",
       "8   0506d5ab-b679-415f-a3ce-40c762a73251   \n",
       "12  0506d5ab-b679-415f-a3ce-40c762a73251   \n",
       "15  0506d5ab-b679-415f-a3ce-40c762a73251   \n",
       "21  0506d5ab-b679-415f-a3ce-40c762a73251   \n",
       "\n",
       "                                         affiliations funding  ...  \\\n",
       "7   {'f05e4224-11eb-4c77-b9be-917a55eeb9b1': {'cou...     NaN  ...   \n",
       "8   {'fcb9d5e2-a57f-45fa-b650-4a1a1e3d11fd': {'cou...     NaN  ...   \n",
       "12  {'d1361bd1-ee0f-4550-99f9-4b6939fccfcd': {'cou...     NaN  ...   \n",
       "15  {'f05e4224-11eb-4c77-b9be-917a55eeb9b1': {'cou...     NaN  ...   \n",
       "21  {'f05e4224-11eb-4c77-b9be-917a55eeb9b1': {'cou...     NaN  ...   \n",
       "\n",
       "                                           references num_citations  \\\n",
       "7   51a23b45-b324-409a-85db-22bb9d5e25e5;51a23b45-...            38   \n",
       "8                                                 NaN            67   \n",
       "12  f49acf4d-1287-499e-bb1f-a571a7b4caa7;e09fc743-...            87   \n",
       "15                                                NaN            23   \n",
       "21  ad371947-f480-41cf-8e2b-256548e91e6b;51a23b45-...            90   \n",
       "\n",
       "   num_references                               clean_abstract_title cluster  \\\n",
       "7             9.0  preprocessing steps normalization handling mis...       4   \n",
       "8             0.0  feature engineering plays crucial role improvi...       7   \n",
       "12           70.0  cybersecurity frameworks nist guidelines risk ...       7   \n",
       "15            1.0  identification family malware specimen belongs...       8   \n",
       "21            4.0  malware dangerous costly cyber threats nationa...       8   \n",
       "\n",
       "                                  cluster_coordinates  \\\n",
       "7   [1.1920929e-07, 1.1920929e-07, 1.1920929e-07, ...   \n",
       "8   [1.1920929e-07, 1.1920929e-07, 1.1920929e-07, ...   \n",
       "12  [1.1920929e-07, 0.30378982, 1.1920929e-07, 1.1...   \n",
       "15  [0.43752947, 1.1920929e-07, 1.1920929e-07, 1.1...   \n",
       "21  [0.98035914, 1.1920929e-07, 1.1920929e-07, 0.2...   \n",
       "\n",
       "   similarity_to_cluster_centroid  \\\n",
       "7                        1.336318   \n",
       "8                        1.043619   \n",
       "12                       1.013045   \n",
       "15                       0.854075   \n",
       "21                       1.166755   \n",
       "\n",
       "                                       title_abstract  embed_prune llm_prune  \n",
       "7   Using GANs for Realistic Image Synthesis Graph...         True      True  \n",
       "8   The Intersection of AI and IoT in Smart Cities...         True      True  \n",
       "12  Neural Architecture Search for Optimized Model...         True      True  \n",
       "15  Unraveling the Mystery of Black Box AI Identif...         True      True  \n",
       "21  Unraveling the Mystery of Black Box AI Explori...         True      True  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_artic_fox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
