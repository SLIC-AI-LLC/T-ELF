{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iPenguin - Semantic Scholar Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows async co-routines to work inside of jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHING_DIR = 's2_cache'\n",
    "CACHING_DIR = pathlib.Path(CACHING_DIR).resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create the S2 Handler Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.pre_processing.iPenguin.SemanticScholar import SemanticScholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found S2_KEY environment variable\n"
     ]
    }
   ],
   "source": [
    "if \"S2_KEY\" in os.environ:\n",
    "    print(\"Found S2_KEY environment variable\")\n",
    "    API_KEY = os.environ[\"S2_KEY\"]\n",
    "else:\n",
    "    print(\"Variable does not exist. Export SemanticScholar API key on your environment using the variable name S2_KEY.\")\n",
    "    API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = SemanticScholar(\n",
    "    key = API_KEY,\n",
    "    mode = 'fs',         # file system caching mode (default)\n",
    "    name = CACHING_DIR,  # where to cache the files\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use the S2 Handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Lookup Papers by their ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAPERS = [\n",
    "    '59e4d6475c41096befeafec55ea5ad97432de527', \n",
    "    '9806df234e723cd348112f348ee0724f52bc8f73', \n",
    "]\n",
    "\n",
    "len(PAPERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[S2]: Found 2 papers in 0.46s\n"
     ]
    }
   ],
   "source": [
    "count = s2.count(PAPERS, mode='paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2 entries, 0 to 1\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   s2id            2 non-null      object\n",
      " 1   doi             2 non-null      object\n",
      " 2   year            2 non-null      int64 \n",
      " 3   title           2 non-null      object\n",
      " 4   abstract        2 non-null      object\n",
      " 5   s2_authors      2 non-null      object\n",
      " 6   s2_author_ids   2 non-null      object\n",
      " 7   citations       2 non-null      object\n",
      " 8   references      2 non-null      object\n",
      " 9   num_citations   2 non-null      int64 \n",
      " 10  num_references  2 non-null      int64 \n",
      "dtypes: int64(3), object(8)\n",
      "memory usage: 308.0+ bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[S2]: Finished downloading 2 papers for given query in 1.88s\n",
      "[Parallel(n_jobs=2)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "df, paper_ids = s2.search(PAPERS, mode='paper')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['59e4d6475c41096befeafec55ea5ad97432de527',\n",
       " '9806df234e723cd348112f348ee0724f52bc8f73']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_ids[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Get Papers by Author ID(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTHORS = [\n",
    "    '2025666',     # Boian Alexandrov\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the **maximum** number of papers that will be returned from this search. Note that Semantic Scholar has no support for quickly finding the total number of papers for a group of authors. The count is computed by taking the sum of the number of papers for each author. This means that the same paper will be counted multiple times if the authors being examined are co-authors. Use the output of ```SemanticScholar.count(*, mode='author')``` as an upper bound on how many papers will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[S2]: Found 148 papers in 0.54s\n"
     ]
    }
   ],
   "source": [
    "count = s2.count(AUTHORS, mode='author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [00:05<00:00, 27.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 148 entries, 0 to 147\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   s2id            148 non-null    object\n",
      " 1   doi             135 non-null    object\n",
      " 2   year            148 non-null    int64 \n",
      " 3   title           148 non-null    object\n",
      " 4   abstract        102 non-null    object\n",
      " 5   s2_authors      148 non-null    object\n",
      " 6   s2_author_ids   148 non-null    object\n",
      " 7   citations       110 non-null    object\n",
      " 8   references      117 non-null    object\n",
      " 9   num_citations   148 non-null    int64 \n",
      " 10  num_references  148 non-null    int64 \n",
      "dtypes: int64(3), object(8)\n",
      "memory usage: 12.8+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[S2]: Finished downloading 148 papers for given query in 6.35s\n",
      "[Parallel(n_jobs=148)]: Using backend ThreadingBackend with 148 concurrent workers.\n",
      "[Parallel(n_jobs=148)]: Done   2 out of 148 | elapsed:    0.0s remaining:    1.9s\n",
      "[Parallel(n_jobs=148)]: Done 148 out of 148 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "df, paper_ids = s2.search(AUTHORS, mode='author')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Get Papers by Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish how many papers can be found for some query on S2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[S2]: Found 1,213,639 papers in 1.07s\n"
     ]
    }
   ],
   "source": [
    "count = s2.count('tensor decomposition', mode='query')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```SemanticScholar.search()``` function takes an optional argument ```n```. This argument sets an upper limit on how many papers to download. SemanticScholar search ranks the results by relevancy so the top ```n``` most \"relevant\" papers are found for a given query. By default ```n``` is set to 100 but it can be set to any positive integer value. Set ```n``` equal to 0 download all available papers for a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/barron/miniconda3/envs/dev_artic_fox/lib/python3.11/site-packages/TELF/pre_processing/iPenguin/SemanticScholar/s2.py:253: RuntimeWarning: [S2]: Found 1213639 papers for query but limit is set at {n}\n",
      "  warnings.warn(f'[S2]: Found {num_papers} papers for query but limit is set ' \\\n",
      "100%|██████████| 5/5 [00:02<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   s2id            5 non-null      object\n",
      " 1   doi             4 non-null      object\n",
      " 2   year            5 non-null      int64 \n",
      " 3   title           5 non-null      object\n",
      " 4   abstract        5 non-null      object\n",
      " 5   s2_authors      5 non-null      object\n",
      " 6   s2_author_ids   5 non-null      object\n",
      " 7   citations       5 non-null      object\n",
      " 8   references      5 non-null      object\n",
      " 9   num_citations   5 non-null      int64 \n",
      " 10  num_references  5 non-null      int64 \n",
      "dtypes: int64(3), object(8)\n",
      "memory usage: 572.0+ bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[S2]: Finished downloading 5 papers for given query in 3.77s\n",
      "[Parallel(n_jobs=5)]: Using backend ThreadingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "df, paper_ids = s2.search('tensor decomposition', mode='query', n=5)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get all Cached Papers and Form Single DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```SemanticScholar.get_df``` method can be used to access the file system cache directly. This function also takes an optional argument ```targets```. If None (the default) all papers in the cache directory are returned. Otherwise ```targets``` is expected to be an iterable (list, set, tuple) of Semantic Scholar IDs to be fetched from the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 153 entries, 0 to 152\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   s2id            153 non-null    object\n",
      " 1   doi             139 non-null    object\n",
      " 2   year            153 non-null    int64 \n",
      " 3   title           153 non-null    object\n",
      " 4   abstract        107 non-null    object\n",
      " 5   s2_authors      153 non-null    object\n",
      " 6   s2_author_ids   153 non-null    object\n",
      " 7   citations       115 non-null    object\n",
      " 8   references      122 non-null    object\n",
      " 9   num_citations   153 non-null    int64 \n",
      " 10  num_references  153 non-null    int64 \n",
      "dtypes: int64(3), object(8)\n",
      "memory usage: 13.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df, paper_ids = SemanticScholar.get_df(CACHING_DIR)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_artic_fox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
