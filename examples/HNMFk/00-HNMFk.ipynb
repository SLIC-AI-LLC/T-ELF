{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "nnn = 1\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(nnn) # export OMP_NUM_THREADS=1\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(nnn) # export OPENBLAS_NUM_THREADS=1\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(nnn) # export MKL_NUM_THREADS=1\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = str(nnn) # export VECLIB_MAXIMUM_THREADS=1\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(nnn)  # export NUMEXPR_NUM_THREADS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.factorization.HNMFk import HNMFk\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(os.path.join(\"..\", \"..\", \"scripts\"))\n",
    "from generate_X import gen_data,gen_data_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.40'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import TELF\n",
    "TELF.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maksim/Desktop/Code/telf_internal/examples/HNMFk/../../scripts/generate_X.py:60: UserWarning: Kruskal's theorem probably won't apply, may not have a unique nCPD.\n",
      "  warnings.warn(\"Kruskal's theorem probably won't apply, may not have a unique nCPD.\")\n"
     ]
    }
   ],
   "source": [
    "Xsp = gen_data_sparse(shape=[500, 500], density=0.01)[\"X\"]\n",
    "X = gen_data(R=4, shape=[500, 500])[\"X\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = np.arange(1, 10, 1)\n",
    "perts = 2\n",
    "iters = 1000\n",
    "eps = 0.015\n",
    "init = \"nnsvd\"\n",
    "save_path = \"HNMFk_results_path\"\n",
    "name = \"example_HNMFk3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmfk_params = {\n",
    "    \"n_perturbs\":perts,\n",
    "    \"n_iters\":iters,\n",
    "    \"epsilon\":eps,\n",
    "    \"n_jobs\":2,\n",
    "    \"init\":init, \n",
    "    \"use_gpu\":False,\n",
    "    \"save_path\":save_path, \n",
    "    \"predict_k_method\":\"sill\",\n",
    "    \"verbose\":False,\n",
    "    \"nmf_verbose\":False,\n",
    "    \"transpose\":False,\n",
    "    \"sill_thresh\":0.8,\n",
    "    \"pruned\":True,\n",
    "    'nmf_method':'nmf_fro_mu',\n",
    "    \"calculate_error\":False,\n",
    "    \"use_consensus_stopping\":0,\n",
    "    \"calculate_pac\":False,\n",
    "    \"consensus_mat\":False,\n",
    "    \"perturb_type\":\"uniform\",\n",
    "    \"perturb_multiprocessing\":False,\n",
    "    \"perturb_verbose\":False,\n",
    "    \"simple_plot\":True,\n",
    "    \"k_search_method\":\"bst_post\",\n",
    "    \"H_sill_thresh\":0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hnmfk_params = {\n",
    "    # we can specify nmfk parameters for each depth, or use same for all depth\n",
    "    # below will use the same nmfk parameters for all depths\n",
    "    # when using for each depth, append to the list \n",
    "    # for example, [nmfk_params0, nmfk_params1, nmfk_params2] for depth of 2\n",
    "    \"nmfk_params\": [nmfk_params], \n",
    "    # where to perform clustering, can be W or H\n",
    "    # if W, row of X should be samples\n",
    "    # if H, columns of X should be samples\n",
    "    \"cluster_on\":\"H\",\n",
    "    # how deep to go in each topic after root node\n",
    "    # if -1, it goes until samples cannot be seperated further\n",
    "    \"depth\":3,\n",
    "    # stopping criteria for num of samples\n",
    "    \"sample_thresh\":100,\n",
    "    # if K2=True, decomposition is done only for k=2 instead of \n",
    "    # finding and predicting the number of stable latent features\n",
    "    \"K2\":False,\n",
    "    # after first nmfk, when selecting Ks search range, minimum k to start\n",
    "    \"Ks_deep_min\":1,\n",
    "    # After first nmfk, when selecting Ks search range, maximum k to try.\n",
    "    # When None, maximum k will be same as k selected for parent node.\n",
    "    \"Ks_deep_max\": 20,\n",
    "    # after first nmfk, when selecting Ks search range, k step size\n",
    "    \"Ks_deep_step\":1,\n",
    "    # where to save\n",
    "    \"experiment_name\":os.path.join(\"results\", name),\n",
    "    # when True, names the nodes randomly.\n",
    "    # When False, uses k index for ancestry naming\n",
    "    \"random_identifiers\":False,\n",
    "    # What naming convention to be used for root node.\n",
    "    \"root_node_name\":\"Root\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maksim/Desktop/Code/telf_internal/TELF/factorization/HNMFk.py:321: UserWarning: No checkpoint file found!\n",
      "  warnings.warn(\"No checkpoint file found!\")\n",
      "/Users/maksim/Desktop/Code/telf_internal/TELF/factorization/NMFk.py:838: UserWarning: predict_k_method is defaulted to WH_sill!\n",
      "  warnings.warn(\"predict_k_method is defaulted to WH_sill!\")\n",
      "/Users/maksim/miniconda3/envs/TELF/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/maksim/miniconda3/envs/TELF/lib/python3.11/site-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'time': 5.2496418952941895}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HNMFk(**hnmfk_params)\n",
    "model.fit(X, Ks, from_checkpoint=True, save_checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Traverse Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Below functions can be utilized to walk the graph:\n",
    "\n",
    " ```python\n",
    "model.traverse_nodes()\n",
    "model.go_to_root()\n",
    "model.get_node()\n",
    "model.go_to_parent()\n",
    "model.go_to_children(idx:int)\n",
    "model.go_to_node(node_name:str)\n",
    "model.traverse_tiny_leaf_topics(threshold:int)\n",
    "model.process_tiny_leaf_topics(threshold:int)\n",
    "model.get_tiny_leaf_topics()\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can reset the iterator to go back to the root node as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Root'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = model.go_to_root()\n",
    "node[\"node_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " HNMFk class includes a iterator that enables walking the graph nodes. Current node the iterator is at can be obtained as shown below (always starts at root node):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['node_name', 'depth', 'W', 'H', 'k', 'parent_topic', 'parent_node_k', 'parent_node_name', 'child_node_names', 'original_child_node_names', 'original_indices', 'num_samples', 'leaf', 'user_node_data', 'cluster_indices_in_parent', 'node_save_path', 'parent_node_factors_path', 'parent_node_save_path', 'exception', 'signature', 'probabilities', 'centroids', 'factors_path'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = model.get_node()\n",
    "node.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can also see the name of the node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Root'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node[\"node_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " And we can see the child nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Root_0', 'Root_1', 'Root_2', 'Root_3']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node[\"child_node_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can go to the child node specified with an index. For example, to go to the first child, we index at 0. When we go to the child node, it will return the child node and set the iterator to the child node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Root_1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = model.go_to_children(1)\n",
    "node[\"node_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "node[\"factors_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['node_name', 'depth', 'W', 'H', 'k', 'parent_topic', 'parent_node_k', 'parent_node_name', 'child_node_names', 'original_child_node_names', 'original_indices', 'num_samples', 'leaf', 'user_node_data', 'cluster_indices_in_parent', 'node_save_path', 'parent_node_factors_path', 'parent_node_save_path', 'exception', 'signature', 'probabilities', 'centroids', 'factors_path'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/example_HNMFk3/depth_1/Root_1/node_Root_1.p'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node[\"node_save_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'results/example_HNMFk3/depth_0/Root/node_Root.p'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node[\"parent_node_save_path\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to a specific node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Root_3_0', 'Root_3_1', 'Root_3_2', 'Root_3_3']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = model.go_to_node(\"Root_3\")\n",
    "node[\"child_node_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Take a look at the parent node, which should be the root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Root'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node[\"parent_node_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_node() always returns the node we are currently at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Root_3'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = model.get_node()\n",
    "node[\"node_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to parent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Root'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = model.go_to_parent()\n",
    "node[\"node_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From each node, we can get the samples that was clustered in the node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node[\"original_indices\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at which of the parent H clustering indices resulted in the samples in current cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original indices from X= [ 32  38  55  59 140 176 178 204 274 299 305 338 345 413 415 477 485 489\n",
      " 496]\n",
      "H clustering indices from parent topic= [ 27  32  47  50 121 152 153 174 237 260 265 296 303 361 363 415 422 424\n",
      " 430]\n"
     ]
    }
   ],
   "source": [
    "node = model.go_to_node(\"Root_3_0\")\n",
    "print(\"Original indices from X=\", node[\"original_indices\"])\n",
    "print(\"H clustering indices from parent topic=\", node[\"cluster_indices_in_parent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root is empty because it does not have a parent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H clustering indices from parent topic= []\n"
     ]
    }
   ],
   "source": [
    "node = model.go_to_root()\n",
    "print(\"H clustering indices from parent topic=\", node[\"cluster_indices_in_parent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check if a given node in the graph a leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node[\"leaf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Finally, we can obtain all the nodes using the following method. Note that while other other node iterator options above are online, meaning each node is loaded into memory one at a time, the following traversal will load all nodes into the memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nodes = model.traverse_nodes()\n",
    "len(all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "leaf_nodes = []\n",
    "for node in all_nodes:\n",
    "    if node[\"leaf\"]:\n",
    "        indices += list(node[\"original_indices\"])\n",
    "        leaf_nodes.append(node)\n",
    "indices.sort()\n",
    "assert all(indices == np.arange(0, X.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(leaf_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['node_name', 'depth', 'W', 'H', 'k', 'parent_topic', 'parent_node_k', 'parent_node_name', 'child_node_names', 'original_child_node_names', 'original_indices', 'num_samples', 'leaf', 'user_node_data', 'cluster_indices_in_parent', 'node_save_path', 'parent_node_factors_path', 'parent_node_save_path', 'exception', 'signature', 'probabilities', 'centroids', 'factors_path'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf_nodes[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 16)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf_nodes[0][\"centroids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf_nodes[0][\"signature\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaf_nodes[0][\"probabilities\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['node_name', 'depth', 'W', 'H', 'k', 'parent_topic', 'parent_node_k', 'parent_node_name', 'child_node_names', 'original_child_node_names', 'original_indices', 'num_samples', 'leaf', 'user_node_data', 'cluster_indices_in_parent', 'node_save_path', 'parent_node_factors_path', 'parent_node_save_path', 'exception', 'signature', 'probabilities', 'centroids', 'factors_path'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.go_to_node(\"Root_2\").keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the graph for nodes with small number of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at outlier clusters where the number of documents are less than the given threshold at the leafs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_name= Root_3_1\n",
      "num_samples= 2\n",
      "leaf= True\n",
      "parent_node_name= Root_3\n",
      "---------\n",
      "node_name= Root_3_3_0\n",
      "num_samples= 3\n",
      "leaf= True\n",
      "parent_node_name= Root_3_3\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "threshold = 5\n",
    "\n",
    "for node in model.traverse_tiny_leaf_topics(threshold):\n",
    "    print(\"node_name=\", node[\"node_name\"])\n",
    "    print(\"num_samples=\", node[\"num_samples\"])\n",
    "    print(\"leaf=\", node[\"leaf\"])\n",
    "    print(\"parent_node_name=\", node[\"parent_node_name\"])\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above operation for traversing the graph did not make any changes to the graph. We can look at the parent of these tiny nodes to see that these nodes are still there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny node name=  Root_3_1\n",
      "Child nodes of the parent of tiny node=  ['Root_3_0', 'Root_3_1', 'Root_3_2', 'Root_3_3']\n",
      "---------\n",
      "Tiny node name=  Root_3_3_0\n",
      "Child nodes of the parent of tiny node=  ['Root_3_3_0', 'Root_3_3_1', 'Root_3_3_2', 'Root_3_3_3']\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "for node in model.traverse_tiny_leaf_topics(threshold):\n",
    "    print(\"Tiny node name= \", node[\"node_name\"])\n",
    "    print(\"Child nodes of the parent of tiny node= \", model.go_to_node(node[\"parent_node_name\"])[\"child_node_names\"])\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also process the graph to remove these nodes, and save them seperatly. If we try to load these nodes now, it will give an error because we have not run the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Root': 'results/example_HNMFk3/depth_0/Root/node_Root.p',\n",
       " 'Root_0': 'results/example_HNMFk3/depth_1/Root_0/node_Root_0.p',\n",
       " 'Root_1': 'results/example_HNMFk3/depth_1/Root_1/node_Root_1.p',\n",
       " 'Root_2': 'results/example_HNMFk3/depth_1/Root_2/node_Root_2.p',\n",
       " 'Root_3': 'results/example_HNMFk3/depth_1/Root_3/node_Root_3.p',\n",
       " 'Root_3_0': 'results/example_HNMFk3/depth_2/Root_3_0/node_Root_3_0.p',\n",
       " 'Root_3_1': 'results/example_HNMFk3/depth_2/Root_3_1/node_Root_3_1.p',\n",
       " 'Root_3_2': 'results/example_HNMFk3/depth_2/Root_3_2/node_Root_3_2.p',\n",
       " 'Root_3_3': 'results/example_HNMFk3/depth_2/Root_3_3/node_Root_3_3.p',\n",
       " 'Root_3_3_0': 'results/example_HNMFk3/depth_3/Root_3_3_0/node_Root_3_3_0.p',\n",
       " 'Root_3_3_1': 'results/example_HNMFk3/depth_3/Root_3_3_1/node_Root_3_3_1.p',\n",
       " 'Root_3_3_2': 'results/example_HNMFk3/depth_3/Root_3_3_2/node_Root_3_3_2.p',\n",
       " 'Root_3_3_3': 'results/example_HNMFk3/depth_3/Root_3_3_3/node_Root_3_3_3.p'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.node_save_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load the tiny leafs. Did you call process_tiny_leaf_topics(threshold:int)? [Errno 2] No such file or directory: 'results/example_HNMFk3/tiny_leafs.p'\n"
     ]
    }
   ],
   "source": [
    "tiny_leafs = model.get_tiny_leaf_topics()\n",
    "tiny_leafs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process the graph first then to seperate these tiny nodes based on the given threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_leafs = model.process_tiny_leaf_topics(threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_name= Root_3_1\n",
      "num_samples= 2\n",
      "leaf= True\n",
      "parent_node_name= Root_3\n",
      "---------\n",
      "node_name= Root_3_3_0\n",
      "num_samples= 3\n",
      "leaf= True\n",
      "parent_node_name= Root_3_3\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "for node in tiny_leafs:\n",
    "    print(\"node_name=\", node[\"node_name\"])\n",
    "    print(\"num_samples=\", node[\"num_samples\"])\n",
    "    print(\"leaf=\", node[\"leaf\"])\n",
    "    print(\"parent_node_name=\", node[\"parent_node_name\"])\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also directly load them again without pre-processing the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_leafs = model.get_tiny_leaf_topics()\n",
    "len(tiny_leafs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are saved in a pickle file named ```tiny_leafs.p```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint.p \u001b[34mdepth_0\u001b[m\u001b[m      \u001b[34mdepth_1\u001b[m\u001b[m      \u001b[34mdepth_2\u001b[m\u001b[m      \u001b[34mdepth_3\u001b[m\u001b[m      tiny_leafs.p\n"
     ]
    }
   ],
   "source": [
    "! ls $model.experiment_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the parent node for these nodes now, their child node list should not have the removed tiny nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny node name=  Root_3_1\n",
      "Child nodes of the parent of tiny node=  ['Root_3_0', 'Root_3_2', 'Root_3_3']\n",
      "---------\n",
      "Tiny node name=  Root_3_3_0\n",
      "Child nodes of the parent of tiny node=  ['Root_3_3_1', 'Root_3_3_2', 'Root_3_3_3']\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "for node in tiny_leafs:\n",
    "    print(\"Tiny node name= \", node[\"node_name\"])\n",
    "    print(\"Child nodes of the parent of tiny node= \", model.go_to_node(node[\"parent_node_name\"])[\"child_node_names\"])\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now try to traverse the graph for these tiny nodes, we should not get any because they are removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_leafs_now = model.traverse_tiny_leaf_topics(threshold=threshold)\n",
    "tiny_leafs_now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot directly access these tiny nodes with the graph iterator anymore since they are not listed in any child node of a node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node not found!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.go_to_node(tiny_leafs[0][\"node_name\"])[\"node_name\"]\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also re-process the graph with different treshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_name= Root_3_1\n",
      "num_samples= 2\n",
      "leaf= True\n",
      "parent_node_name= Root_3\n",
      "---------\n",
      "node_name= Root_3_3_0\n",
      "num_samples= 3\n",
      "leaf= True\n",
      "parent_node_name= Root_3_3\n",
      "---------\n",
      "node_name= Root_3_3_2\n",
      "num_samples= 11\n",
      "leaf= True\n",
      "parent_node_name= Root_3_3\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "threshold=15\n",
    "tiny_leafs = model.process_tiny_leaf_topics(threshold=threshold)\n",
    "\n",
    "for node in tiny_leafs:\n",
    "    print(\"node_name=\", node[\"node_name\"])\n",
    "    print(\"num_samples=\", node[\"num_samples\"])\n",
    "    print(\"leaf=\", node[\"leaf\"])\n",
    "    print(\"parent_node_name=\", node[\"parent_node_name\"])\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_leafs = model.get_tiny_leaf_topics()\n",
    "len(tiny_leafs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny node name=  Root_3_1\n",
      "Child nodes of the parent of tiny node=  ['Root_3_0', 'Root_3_2', 'Root_3_3']\n",
      "---------\n",
      "Tiny node name=  Root_3_3_0\n",
      "Child nodes of the parent of tiny node=  ['Root_3_3_1', 'Root_3_3_3']\n",
      "---------\n",
      "Tiny node name=  Root_3_3_2\n",
      "Child nodes of the parent of tiny node=  ['Root_3_3_1', 'Root_3_3_3']\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "for node in tiny_leafs:\n",
    "    print(\"Tiny node name= \", node[\"node_name\"])\n",
    "    print(\"Child nodes of the parent of tiny node= \", model.go_to_node(node[\"parent_node_name\"])[\"child_node_names\"])\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the graph to add these back. Simply set the threshold to be ```None```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.process_tiny_leaf_topics(threshold=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these nodes are added back, we can traverse the graph and they will be found, and their parents will have the name of those tiny nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny node name=  Root_3_1\n",
      "Child nodes of the parent of tiny node=  ['Root_3_0', 'Root_3_1', 'Root_3_2', 'Root_3_3']\n",
      "---------\n",
      "Tiny node name=  Root_3_3_0\n",
      "Child nodes of the parent of tiny node=  ['Root_3_3_0', 'Root_3_3_1', 'Root_3_3_2', 'Root_3_3_3']\n",
      "---------\n",
      "Tiny node name=  Root_3_3_2\n",
      "Child nodes of the parent of tiny node=  ['Root_3_3_0', 'Root_3_3_1', 'Root_3_3_2', 'Root_3_3_3']\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "for node in model.traverse_tiny_leaf_topics(threshold):\n",
    "    print(\"Tiny node name= \", node[\"node_name\"])\n",
    "    print(\"Child nodes of the parent of tiny node= \", model.go_to_node(node[\"parent_node_name\"])[\"child_node_names\"])\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also see the saved tiny nodes are no longer available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load the tiny leafs. Did you call process_tiny_leaf_topics(threshold:int)? [Errno 2] No such file or directory: 'results/example_HNMFk3/tiny_leafs.p'\n"
     ]
    }
   ],
   "source": [
    "tiny_leafs = model.get_tiny_leaf_topics()\n",
    "tiny_leafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint.p \u001b[34mdepth_0\u001b[m\u001b[m      \u001b[34mdepth_1\u001b[m\u001b[m      \u001b[34mdepth_2\u001b[m\u001b[m      \u001b[34mdepth_3\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls $model.experiment_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading checkpoint to have access to the graph iterator and the node objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved object state from checkpoint...\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "model = HNMFk(experiment_name=os.path.join(\"results\", name))\n",
    "model.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Root': 'results/example_HNMFk3/depth_0/Root/node_Root.p',\n",
       " 'Root_0': 'results/example_HNMFk3/depth_1/Root_0/node_Root_0.p',\n",
       " 'Root_1': 'results/example_HNMFk3/depth_1/Root_1/node_Root_1.p',\n",
       " 'Root_2': 'results/example_HNMFk3/depth_1/Root_2/node_Root_2.p',\n",
       " 'Root_3': 'results/example_HNMFk3/depth_1/Root_3/node_Root_3.p',\n",
       " 'Root_3_0': 'results/example_HNMFk3/depth_2/Root_3_0/node_Root_3_0.p',\n",
       " 'Root_3_1': 'results/example_HNMFk3/depth_2/Root_3_1/node_Root_3_1.p',\n",
       " 'Root_3_2': 'results/example_HNMFk3/depth_2/Root_3_2/node_Root_3_2.p',\n",
       " 'Root_3_3': 'results/example_HNMFk3/depth_2/Root_3_3/node_Root_3_3.p',\n",
       " 'Root_3_3_0': 'results/example_HNMFk3/depth_3/Root_3_3_0/node_Root_3_3_0.p',\n",
       " 'Root_3_3_1': 'results/example_HNMFk3/depth_3/Root_3_3_1/node_Root_3_3_1.p',\n",
       " 'Root_3_3_2': 'results/example_HNMFk3/depth_3/Root_3_3_2/node_Root_3_3_2.p',\n",
       " 'Root_3_3_3': 'results/example_HNMFk3/depth_3/Root_3_3_3/node_Root_3_3_3.p'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.node_save_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['node_name', 'depth', 'W', 'H', 'k', 'parent_topic', 'parent_node_k', 'parent_node_name', 'child_node_names', 'original_child_node_names', 'original_indices', 'num_samples', 'leaf', 'user_node_data', 'cluster_indices_in_parent', 'node_save_path', 'parent_node_factors_path', 'parent_node_save_path', 'exception', 'signature', 'probabilities', 'centroids', 'factors_path'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.go_to_node(\"Root_0\").keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Root',\n",
       " dict_keys(['node_name', 'depth', 'W', 'H', 'k', 'parent_topic', 'parent_node_k', 'parent_node_name', 'child_node_names', 'original_child_node_names', 'original_indices', 'num_samples', 'leaf', 'user_node_data', 'cluster_indices_in_parent', 'node_save_path', 'parent_node_factors_path', 'parent_node_save_path', 'exception', 'signature', 'probabilities', 'centroids', 'factors_path']))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = model.go_to_node(list(model.node_save_paths.keys())[0])\n",
    "node[\"node_name\"], node.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the nodes that are not completed yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.target_jobs.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TELF_public",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
