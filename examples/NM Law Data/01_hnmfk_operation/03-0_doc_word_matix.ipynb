{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Document-Word Matrix Construction for Legal Topic Modeling\n",
    "\n",
    "This notebook is a core part of the **NM Law Data Pipeline**. It transforms cleaned legal texts into a **document-word matrix (DWM)** suitable for unsupervised topic modeling via **Hierarchical Nonnegative Matrix Factorization (HNMFk)** and related methods.\n",
    "\n",
    "---\n",
    "\n",
    "##  Purpose\n",
    "\n",
    "Legal texts—such as statutes, constitutional clauses, and case opinions—are rich in content but complex in structure. Before they can be used for clustering, topic modeling, or graph generation, they must be transformed into a numeric format that preserves semantic meaning.\n",
    "\n",
    "This notebook performs that transformation by:\n",
    "1. **Loading** cleaned legal data from prior scraping and formatting steps,\n",
    "2. **Cleaning and consolidating** the vocabulary using domain-specific text processors,\n",
    "3. **Extracting a refined vocabulary**, and\n",
    "4. **Constructing a sparse matrix** mapping each document to the frequency of its terms.\n",
    "\n",
    "---\n",
    "---\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Paths \n",
    "\n",
    "This section initializes the file system paths required for reading the structured CSVs and saving intermediate results. These paths should point to:\n",
    "\n",
    "- Preprocessed legal documents (e.g., statutes, cases),\n",
    "- Vocabulary consolidation tools and configuration,\n",
    "- Output directories for document-word matrices and vocabularies.\n",
    "\n",
    "These paths form the backbone of this pipeline stage and must be consistent across notebook runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, os\n",
    "\n",
    "CSV_PATH = None \n",
    "RESULTS_DIR = pathlib.Path('./results')\n",
    "RESULTS_DIR.resolve().mkdir(parents=True, exist_ok=True)\n",
    "RESULTS_FILE = pathlib.Path('operated_documents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "### Define column to extract as documents\n",
    "\n",
    "Loads the cleaned and structured legal texts generated in earlier pipeline steps. You must specify:\n",
    "\n",
    "- The target CSV file (e.g., `STATUTE.csv`, `SUPREME.csv`, etc.),\n",
    "- The column containing the primary text data (usually `content`, `provision_text`, or `opinion_text`).\n",
    "\n",
    "This column is extracted and loaded into memory for vocabulary construction and matrix generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, pickle\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df['title_abs'] = df['title'] + ' ' + df['abstract']\n",
    "df = df.dropna(subset=['title_abs'])\n",
    "df.info()\n",
    "documents = df.title_abs.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary refinement through cleaning\n",
    "\n",
    "This step standardizes and cleans the raw text using Vulture—a modular pre-processing framework.\n",
    "\n",
    "Tasks performed include:\n",
    "- Lowercasing,\n",
    "- Removing non-alphanumeric tokens,\n",
    "- Lemmatization,\n",
    "- Dropping stopwords and non-English phrases,\n",
    "- Optional filtering of frequent or infrequent terms.\n",
    "\n",
    "This ensures that the vocabulary used in downstream matrix decomposition is domain-specific, relevant, and noise-free.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.pre_processing import Vulture\n",
    "from TELF.pre_processing.Vulture.modules import AcronymDetector\n",
    "from TELF.pre_processing.Vulture.modules import SimpleCleaner\n",
    "from TELF.pre_processing.Vulture.default_stop_words import STOP_WORDS\n",
    "from TELF.pre_processing.Vulture.default_stop_phrases import STOP_PHRASES\n",
    "from TELF.pre_processing import Beaver\n",
    "from TELF.pre_processing.Vulture.tokens_analysis.vocab_consolidator import VocabularyConsolidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vulture = Vulture(n_jobs  = -1, \n",
    "                  verbose = 10,  # Disable == 0, Verbose >= 1\n",
    "                 )\n",
    "steps = [SimpleCleaner( min_characters=3,\n",
    "                        stop_words = STOP_WORDS,\n",
    "                        stop_phrases = STOP_PHRASES,\n",
    "                        order = [\n",
    "                                'remove_numbers',\n",
    "                                'standardize_hyphens',\n",
    "                                'remove_stop_phrases',\n",
    "                                'isolate_frozen',\n",
    "                                'remove_copyright_statement',\n",
    "                                'make_lower_case',\n",
    "                                'remove_formulas',\n",
    "                                'normalize',\n",
    "                                'remove_next_line',\n",
    "                                'remove_email',\n",
    "                                'remove_()',\n",
    "                                'remove_[]',\n",
    "                                'remove_special_characters',\n",
    "                                'remove_nonASCII_boundary',\n",
    "                                'remove_nonASCII',\n",
    "                                'remove_tags',\n",
    "                                'remove_stop_words',\n",
    "                                'remove_standalone_numbers',\n",
    "                                'remove_extra_whitespace',\n",
    "                                'min_characters',\n",
    "        ])]\n",
    "\n",
    "CLEAN_DOCS = os.path.join(RESULTS_DIR, \"clean_documents\")\n",
    "\n",
    "vulture.clean(  documents, \n",
    "                steps=steps,\n",
    "                save_path=CLEAN_DOCS)         \n",
    "\n",
    "clean_documents = pickle.load(open(CLEAN_DOCS, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build acronyms\n",
    "\n",
    "\n",
    "Constructs a mapping of common acronyms found in the legal text (e.g., \"UCC\", \"SCOTUS\") to their expanded forms.\n",
    "\n",
    "This improves semantic resolution by:\n",
    "- Avoiding misleading token frequency boosts for acronyms,\n",
    "- Linking short forms to their original terms in the matrix.\n",
    "\n",
    "Useful for clustering tasks and document understanding where expanded terms carry more meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPERATION_RESULTS = (RESULTS_DIR / RESULTS_FILE)\n",
    "OPERATION_RESULTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "vulture.operate(clean_documents, steps=[AcronymDetector(replace_raw=True)], save_path=RESULTS_DIR, file_name=RESULTS_FILE)               \n",
    "operated_documents = pickle.load(open((OPERATION_RESULTS / '_AcronymDetector.p'), 'rb'))\n",
    "\n",
    "def to_df(documents, operated_documents):\n",
    "    data = {\n",
    "        'id': [],\n",
    "        'text': [],\n",
    "        'acronyms': [],\n",
    "        'acronym_replaced_text': [],\n",
    "    }\n",
    "    for _id, text in documents.items():\n",
    "        data['id'].append(_id)\n",
    "        data['text'].append(text)\n",
    "\n",
    "        data['acronyms'].append(operated_documents.get(_id).get('Acronyms'))\n",
    "        data['acronym_replaced_text'].append(operated_documents.get(_id).get('replaced_text'))\n",
    "    return pd.DataFrame.from_dict(data)\n",
    "\n",
    "    \n",
    "df = to_df(documents, operated_documents)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitutions = {}\n",
    "for id, acronym_data in operated_documents.items():\n",
    "    for src_txt, acronym in acronym_data['Acronyms'].items():\n",
    "        # print(src_txt, acronym)\n",
    "        sub_to = '_'.join(src_txt.split())\n",
    "        substitutions[src_txt] = sub_to\n",
    "        substitutions[acronym] = sub_to\n",
    "\n",
    "for src, sub in substitutions.items():\n",
    "    print(f'{src} : {sub}')\n",
    "\n",
    "from TELF.pre_processing.Vulture.modules import SubstitutionCleaner\n",
    "initial_sub = SubstitutionCleaner(substitutions, permute=False, lower=True, lemmatize=False)\n",
    "step1 = [initial_sub] \n",
    "dataframe_clean_args = {\n",
    "    \"df\": df,\n",
    "    \"steps\": step1,\n",
    "    \"columns\": ['text',],\n",
    "    \"append_to_original_df\": True,\n",
    "    \"concat_cleaned_cols\": True,\n",
    "}\n",
    "\n",
    "df = vulture.clean_dataframe(**dataframe_clean_args) \n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidate terms\n",
    "\n",
    "This step refines the vocabulary by merging different lexical variants of the same concept.\n",
    "\n",
    "For example:\n",
    "- Plurals → Singulars (e.g., \"laws\" → \"law\"),\n",
    "- Variants → Canonical forms (e.g., \"defences\" → \"defense\").\n",
    "\n",
    "The goal is to reduce redundancy in the vocabulary and improve topic resolution during matrix factorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACS_RESULT = RESULTS_DIR / 'clean_acronyms.csv'\n",
    "df.to_csv(ACS_RESULT)\n",
    "documents = df.clean_text.to_dict()\n",
    "CONSOLIDATE_PATH = (RESULTS_DIR / 'VOCAB_CONSOLIDATOR')\n",
    "CONSOLIDATE_PATH.mkdir(parents=True,exist_ok=True)\n",
    "CONSOLIDATE_OUT = CONSOLIDATE_PATH / '_SubstitutionOperator.p'\n",
    "\n",
    "consolidator = VocabularyConsolidator()\n",
    "changes_made_file = 'VOCAB_CONSOLIDATOR_changes.csv'\n",
    "changes_made_save_path = (RESULTS_DIR / changes_made_file)\n",
    "o = consolidator.consolidate_terms( vocabulary=None,\n",
    "                                    texts=documents,\n",
    "                                    ignore_pairs=[('','')],\n",
    "                                    changes_made_save_path= changes_made_save_path,\n",
    "                                    operated_text_save_path= str(CONSOLIDATE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore data to df\n",
    "\n",
    "\n",
    "Once the cleaned and consolidated vocabulary is finalized, this section restores it to a structured DataFrame that includes:\n",
    "\n",
    "- Document IDs,\n",
    "- Cleaned document text (tokenized or raw),\n",
    "- Metadata columns, if needed (e.g., title, source, year).\n",
    "\n",
    "This step ensures compatibility with matrix generation functions and enables traceability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_changed = pd.read_csv(changes_made_save_path)\n",
    "substitution_consolidate = pickle.load(open(CONSOLIDATE_OUT, \"rb\"))\n",
    "print(substitution_consolidate.keys())\n",
    "consolidate_data= []\n",
    "for i,k in substitution_consolidate.items():\n",
    "    consolidate_data.append(k.get('replaced_text', ''))\n",
    "df['cleaned_acs_consolidated'] = consolidate_data\n",
    "df.to_csv(RESULTS_DIR/'cleaned_consolidated_supreme.csv', index=False)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Vocabulary\n",
    "\n",
    "\n",
    "Builds the final list of unique vocabulary terms across all cleaned documents.\n",
    "\n",
    "This vocabulary will define the columns of the document-word matrix (DWM), where each term is a dimension in the resulting sparse matrix. This also forms the base for co-occurrence graphs and topic keyword extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.pre_processing import Beaver\n",
    "\n",
    "beaver = Beaver()\n",
    "settings = {\n",
    "    \"dataset\":df,\n",
    "    \"target_column\":\"cleaned_acs_consolidated\",\n",
    "    \"min_df\":50,\n",
    "    \"max_df\":0.7,\n",
    "    'save_path':RESULTS_DIR\n",
    "}\n",
    "\n",
    "vocabulary = beaver.get_vocabulary(**settings)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/Vocabulary.txt') as f:\n",
    "    VOC = [w.strip() for w in f]\n",
    "len(VOC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the document word matrix for decomposition\n",
    "Generates the actual document-word matrix (DWM), where:\n",
    "\n",
    "- **Rows** represent individual legal documents,\n",
    "- **Columns** represent unique vocabulary terms,\n",
    "- **Values** are  TF-IDF scores.\n",
    "\n",
    "This matrix serves as the input for **Hierarchical Nonnegative Matrix Factorization (HNMFk)** or similar decomposition algorithms. The result enables interpretable topic models, legal concept discovery, and semantic clustering of legal texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"dataset\" : df,\n",
    "    \"target_column\" : \"cleaned_acs_consolidated\",\n",
    "    \"options\" : { \"vocabulary\" : VOC },\n",
    "    \"matrix_type\" : \"tfidf\",\n",
    "    \"save_path\" : RESULTS_DIR\n",
    "}\n",
    "\n",
    "beaver.documents_words(**settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HNMFk\n",
    "\n",
    "## Use the generated document-word matrix  to operate the HNMFk code in the local file called `03-1_run_hnmfk.py`\n",
    "\n",
    "### HNMFk is defined in TELF [here](../../HNMFk/00-HNMFk.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oct2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
