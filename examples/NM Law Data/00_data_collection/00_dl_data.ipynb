{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this Legal Data Collection and Preparation Notebook\n",
    "\n",
    "This notebook forms the foundation of the **NM Law Data Pipeline**, a modular system for collecting and preparing legal documents from the State of New Mexico. The goal is to support downstream tasks like topic modeling, retrieval-augmented generation (RAG), and legal knowledge graph construction by providing clean, structured inputs from raw public sources.\n",
    "\n",
    "---\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The legal domain is defined by dense, interdependent, and jurisdiction-specific documents—such as statutes, constitutions, and case law. These documents are largely available online in unstructured formats that are not directly usable for modern machine learning or retrieval systems.\n",
    "\n",
    "This notebook serves to:\n",
    "- **Collect legal texts** from publicly accessible sources (primarily [Justia](https://law.justia.com/new-mexico/)),\n",
    "- **Clean and organize** them into machine-readable structures (e.g., CSVs),\n",
    "- **Standardize formats** across different legal types (statutes, constitutional provisions, court opinions),\n",
    "- **Prepare inputs** for legal topic discovery, model evaluation, and vector-based semantic search.\n",
    "\n",
    "---\n",
    "\n",
    "## Structure\n",
    "\n",
    "The notebook is organized into two primary phases:\n",
    "\n",
    "### 1. **Data Collection**\n",
    "Using the `NewMexicoScraper` class, the notebook scrapes:\n",
    "- **Statutes** – Codified laws by year.\n",
    "- **Constitution** – Foundational articles and sections of NM law.\n",
    "- **Court of Appeals Decisions** – Case opinions from the intermediate appellate court.\n",
    "- **Supreme Court Decisions** – Precedential decisions from the state’s highest court.\n",
    "\n",
    "The scraper is equipped to:\n",
    "- Avoid duplicate downloads,\n",
    "- Follow legal citation patterns and structure,\n",
    "- Respect HTTP headers and delays for responsible scraping.\n",
    "\n",
    "### 2. **Data Formatting**\n",
    "After collection, documents are parsed from their raw formats (usually HTML or JSON) into consistent, flat tabular CSVs using the `JSONToCSVProcessor`. This includes:\n",
    "- Mapping metadata and content fields,\n",
    "- Flattening hierarchical legal structures (e.g., nested statute titles or court metadata),\n",
    "- Saving results for downstream benchmarking, topic modeling, and retrieval evaluation.\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "This section is responsible for initializing and executing the web scraping of legal documents from public sources such as Justia. The scraper collects statutes, constitutional provisions, and court opinions for New Mexico law. Each step below is modularized to allow flexibility, reusability, and reproducibility across different legal document types and years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nm_scraper import NewMexicoScraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection Definitions\n",
    "\n",
    "Before scraping begins, this section sets up the core configuration in the following cell:\n",
    "- Base URL: The root of the legal document source (e.g., Justia’s New Mexico law portal).\n",
    "- Output Directory: The location where downloaded or parsed files will be saved.\n",
    "- URL Cache File: A local record of visited URLs to avoid redundant downloads.\n",
    "- HTTP Headers: Custom headers (e.g., user-agent) for polite and reliable web scraping.\n",
    "- Year Selection: For statutes, the user must specify which year’s legal codes to retrieve. This is necessary because statutes are archived by year on Justia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://law.justia.com/new-mexico/\"\n",
    "OUTPUT = \"output\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; YourScraper/1.0)\"}\n",
    "VISITED_URLS = os.path.join(OUTPUT, \"saved_urls.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the scraping class instance using the definitions\n",
    "Instantiates the NewMexicoScraper class using the previously defined parameters. This object wraps all scraping logic, including request throttling, error handling, and file I/O. It ensures that all legal documents are downloaded consistently, with clear directory structures for each document type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = NewMexicoScraper(BASE_URL, OUTPUT, HEADERS, VISITED_URLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statutes Collection\n",
    "\n",
    "This section uses the scraper instance to collect New Mexico statutes. These laws are organized by year and structured hierarchically. The year must be explicitly defined by the user. This ensures that only relevant and updated legal codes are included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the year to collect \n",
    "statutes_year = \"2024\"  # Change this to the year you want to scrape\n",
    "\n",
    "# overwrite existing files if the documents already exist\n",
    "scraper.scrape_laws_by_year(statutes_year, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constitution Collection\n",
    "\n",
    "Scrapes constitutional provisions of the State of New Mexico. These are typically less frequently updated than statutes or case law, but are foundational to interpreting both. The scraper will navigate to the relevant section of the legal portal and download each article and section systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite existing files if the documents already exist\n",
    "scraper.scrape_constitution(overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cases Collection\n",
    "\n",
    "Downloads court opinions from two branches:\n",
    "- New Mexico Court of Appeals\n",
    "- New Mexico Supreme Court\n",
    "\n",
    "The user must provide a range or list of decision years. Each case will be parsed from the court’s official listings and saved to disk. This step is essential for downstream tasks like precedent prediction, timeline modeling, and historical legal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the years to collect \n",
    "decision_years = [str(y) for y in range(1960, 2026)]\n",
    "# overwrite existing files if the documents already exist\n",
    "scraper.scrape_all_supreme_court(decision_years, overwrite=True)\n",
    "# overwrite existing files if the documents already exist\n",
    "scraper.scrape_all_court_of_appeals(decision_years, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Formatting\n",
    "After collection, legal text data often contains extra formatting, footers, and irrelevant metadata. This section converts the scraped data into structured CSV files that align with the pipeline’s data schema. This prepares the documents for evaluation, topic modeling, and knowledge graph construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from format_csv import JSONToCSVProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Legal Documents into CSVs\n",
    "\n",
    "This section consolidates the formatting of all legal document types—statutes, constitution, appellate cases, and supreme court cases—into a single processing loop using the `JSONToCSVProcessor` class.\n",
    "\n",
    "Each document type has its own format and processing logic:\n",
    "- **Constitution**: Structured hierarchically and processed with `processor_type=2` to account for its article/section format.\n",
    "- **Statutes**: Parsed with `processor_type=1`, structured by titles and sections, and converted into machine-readable CSVs.\n",
    "- **Court of Appeals Cases**: Judicial opinions from the appellate court are parsed with `processor_type=1` to extract metadata and ruling text.\n",
    "- **Supreme Court Cases**: Similar to appeals, processed to extract key fields and structure the full legal opinion text.\n",
    "\n",
    "All documents are saved into their respective CSVs:\n",
    "- `CONSTITUTION.csv`\n",
    "- `STATUTE.csv`\n",
    "- `APPEALS.csv`\n",
    "- `SUPREME.csv`\n",
    "\n",
    "The resulting dataframes are stored in memory under the following names:\n",
    "- `constitution_df`\n",
    "- `statute_df`\n",
    "- `appeals_df`\n",
    "- `supreme_df`\n",
    "\n",
    "This modular loop ensures consistency in formatting while reducing code duplication and simplifying future extensions or batch runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all sources and configurations in a list of dictionaries\n",
    "formats = [\n",
    "    {\"name\": \"constitution\", \"path\": \"CONSTITUTION PATH\", \"csv\": \"CONSTITUTION.csv\", \"type\": 2},\n",
    "    {\"name\": \"statute\", \"path\": \"STATUTE PATH\", \"csv\": \"STATUTE.csv\", \"type\": 1},\n",
    "    {\"name\": \"appeals\", \"path\": \"APPEALS PATH\", \"csv\": \"APPEALS.csv\", \"type\": 1},\n",
    "    {\"name\": \"supreme\", \"path\": \"SUPREME PATH\", \"csv\": \"SUPREME.csv\", \"type\": 1},\n",
    "]\n",
    "dataframes = {}\n",
    "\n",
    "# Loop through each data type and process\n",
    "for item in formats:\n",
    "    processor = JSONToCSVProcessor(root_path=item[\"path\"], output_path=item[\"csv\"])\n",
    "    df = processor.process_json_files_to_csv(processor_type=item[\"type\"])\n",
    "    dataframes[f\"{item['name']}_df\"] = df\n",
    "\n",
    "constitution_df =  dataframes[\"constitution_df\"]\n",
    "statute_df =  dataframes[\"statute_df\"]\n",
    "appeals_df = dataframes[\"appeals_df\"]\n",
    "supreme_df = dataframes[\"supreme_df\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TELF_debug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
