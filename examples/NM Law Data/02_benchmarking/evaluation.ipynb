{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About LLM Output Evaluation for Legal Question Answering\n",
    "\n",
    "This notebook is part of the **NM Law Data Pipeline** and provides tools to systematically benchmark large language models (LLMs) on domain-specific legal questions.\n",
    "\n",
    "---\n",
    "\n",
    "##  Purpose\n",
    "\n",
    "The legal domain demands precision, traceability, and factual accuracy—attributes that LLMs often struggle with, especially when faced with jurisdiction-specific rules or case law. This notebook aims to:\n",
    "\n",
    "- **Compare model outputs** from multiple LLMs using consistent legal questions,\n",
    "- **Score outputs using factuality and relevance metrics**,\n",
    "- **Support ranking and selection** of the best-performing models for downstream legal automation tasks.\n",
    "\n",
    "By evaluating model quality directly on real legal text and benchmark questions, we close the loop between raw generation and measurable reliability.\n",
    "\n",
    "---\n",
    "\n",
    "##  Components\n",
    "\n",
    "1. **Model Access Setup**: Load API keys for OpenAI, Gemini, Claude, or custom models.\n",
    "2. **Prompt Routing**: Send one or more questions to each model using a standardized interface.\n",
    "3. **Output Capture**: Store model answers with metadata for evaluation and record-keeping.\n",
    "4. **Metric Evaluation**: Score each answer using a suite of LLM-targeted metrics (FactCC, ROUGE, SummaC, NLI).\n",
    "5. **Result Interpretation**: Analyze performance across metrics, prompts, or models.\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Multiple models \n",
    "\n",
    "This section enables querying multiple large language models (LLMs) simultaneously with the same legal prompt or question. The goal is to compare model behavior, output quality, and factual consistency across different providers or versions (e.g., GPT-4o, Gemini, Claude, SMART-SLIC, etc.).\n",
    "\n",
    "This is critical for benchmarking LLMs in the legal domain, where subtle variations in phrasing or factual grounding can significantly impact legal reasoning or applicability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define api keys\n",
    "\n",
    "\n",
    "Load or define API keys for each model provider you want to evaluate (e.g., OpenAI, Google, Anthropic). These keys must be set securely and should not be committed to version control.\n",
    "\n",
    "Each model provider may have unique endpoint structures, token limits, and pricing, so defining the keys and routing logic upfront allows the system to handle those differences in a modular way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_KEY = None\n",
    "ANTHROPIC_KEY = None \n",
    "BARD_KEY = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define class instance with keys\n",
    "\n",
    "Initialize a wrapper class or unified interface that allows consistent calling of each LLM, regardless of backend. This class should:\n",
    "- Handle retries and error logging,\n",
    "- Format the prompt as needed by each model,\n",
    "- Normalize outputs for consistent evaluation,\n",
    "- Track metadata such as token counts or model version.\n",
    "\n",
    "This design supports batch evaluations and fair comparison between systems under identical question conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_model_qa import MultiModelLegalQA\n",
    "\n",
    "qa = MultiModelLegalQA(\n",
    "    openai_api_key= OPENAI_KEY,\n",
    "    anthropic_api_key=ANTHROPIC_KEY,\n",
    "    google_bard_api_key=BARD_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask a question for all models to answer\n",
    "\n",
    "Input a single benchmark legal question (e.g., \"What are the conditions under which estoppel applies in New Mexico law?\") and route it through each LLM.\n",
    "\n",
    "The result is a dictionary or list of model outputs, ready for:\n",
    "- Manual inspection,\n",
    "- Quantitative evaluation (FactCC, NLI, ROUGE, etc.),\n",
    "- Dataset generation for fine-tuning or posthoc analysis.\n",
    "\n",
    "This step forms the core of legal question-answer evaluation.\n",
    "\n",
    "### The legal question sets used for the paper can be found in the [58_sme.txt](https://github.com/lanl/T-ELF/tree/main/data/NM_law_questions_and_dates/58_sme.txt) and the [25_system_domain_questions.txt](https://github.com/lanl/T-ELF/tree/main/data/NM_law_questions_and_dates/25_system_domain_questions.txt) in the data directory, which were created and reviewed by a lawyer subject matter expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many New Mexico Supreme Court cases mention ‘Habeas Corpus’?\"\n",
    "\n",
    "answers = qa.ask_all_models(source='', question=question)\n",
    "for model, answer in answers.items():\n",
    "    print(f\"\\n=== {model} ===\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "\n",
    "# Imortant to collect the model outputs into a csv here, defined in the following code block\n",
    "\n",
    "### ___\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the inputs and outputs to Evaluate the models\n",
    "\n",
    "This section structures the evaluation pipeline by defining:\n",
    "- **Inputs**: Model responses, the original question, and optional gold-standard reference answers.\n",
    "- **Metrics**: Chosen to reflect performance on factuality, legal consistency, summarization quality, and truthfulness. Common metrics include:\n",
    "  - **FactCC** (Factual Consistency),\n",
    "  - **ROUGE** (Summarization overlap),\n",
    "  - **SummaC** (Semantic similarity),\n",
    "  - **NLI Entailment** (Natural Language Inference agreement with ground truth).\n",
    "\n",
    "This structured evaluation ensures the outputs are scored consistently and reproducibly, enabling model comparison across dozens of legal prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = None \n",
    "AI_OUTPUT = \"ai_responses\"\n",
    "EVALUATIONS = \"eval.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qa_eval import LegalQuestionAIProcessor\n",
    "processor = LegalQuestionAIProcessor(\n",
    "    cleaned_output_csv=CSV_PATH,\n",
    "    ai_output_dir=AI_OUTPUT,\n",
    "    evaluated_output_csv=EVALUATIONS,\n",
    ")\n",
    "processor.evaluate_responses()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TELF_debug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
