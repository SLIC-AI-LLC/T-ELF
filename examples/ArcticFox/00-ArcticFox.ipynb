{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as ss\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.pre_processing import Vulture\n",
    "from TELF.pre_processing.Vulture.modules import SimpleCleaner\n",
    "from TELF.pre_processing.Vulture.modules import LemmatizeCleaner\n",
    "from TELF.pre_processing.Vulture.modules import RemoveNonEnglishCleaner\n",
    "from TELF.pre_processing.Vulture.default_stop_words import STOP_WORDS\n",
    "from TELF.pre_processing.Vulture.default_stop_phrases import STOP_PHRASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.factorization.HNMFk import HNMFk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.pre_processing import Beaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.post_processing import ArcticFox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   eid               50 non-null     object \n",
      " 1   s2id              50 non-null     object \n",
      " 2   doi               50 non-null     object \n",
      " 3   title             50 non-null     object \n",
      " 4   abstract          50 non-null     object \n",
      " 5   year              50 non-null     int64  \n",
      " 6   authors           50 non-null     object \n",
      " 7   author_ids        50 non-null     object \n",
      " 8   affiliations      50 non-null     object \n",
      " 9   funding           5 non-null      object \n",
      " 10  PACs              8 non-null      object \n",
      " 11  publication_name  50 non-null     object \n",
      " 12  subject_areas     50 non-null     object \n",
      " 13  s2_authors        50 non-null     object \n",
      " 14  s2_author_ids     50 non-null     object \n",
      " 15  citations         45 non-null     object \n",
      " 16  references        38 non-null     object \n",
      " 17  num_citations     50 non-null     int64  \n",
      " 18  num_references    50 non-null     float64\n",
      "dtypes: float64(1), int64(2), object(16)\n",
      "memory usage: 7.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(\"..\", \"..\", \"data\", \"sample2.csv\"))\n",
    "df = df.head(50).reset_index(drop=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    RemoveNonEnglishCleaner(ascii_ratio=0.9, stopwords_ratio=0.25),\n",
    "    SimpleCleaner(stop_words = STOP_WORDS,\n",
    "                  stop_phrases = STOP_PHRASES,\n",
    "                  order = [\n",
    "                      'standardize_hyphens',\n",
    "                      'isolate_frozen',\n",
    "                      'remove_copyright_statement',\n",
    "                      'remove_stop_phrases',\n",
    "                      'make_lower_case',\n",
    "                      'remove_formulas',\n",
    "                      'normalize',\n",
    "                      'remove_next_line',\n",
    "                      'remove_email',\n",
    "                      'remove_()',\n",
    "                      'remove_[]',\n",
    "                      'remove_special_characters',\n",
    "                      'remove_nonASCII_boundary',\n",
    "                      'remove_nonASCII',\n",
    "                      'remove_tags',\n",
    "                      'remove_stop_words',\n",
    "                      'remove_standalone_numbers',\n",
    "                      'remove_extra_whitespace',\n",
    "                      'min_characters',\n",
    "                  ]\n",
    "                 ),\n",
    "    LemmatizeCleaner('spacy'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Vulture]: Cleaning 50 documents\n",
      "  0%|          | 0/1 [00:00<?, ?it/s][Vulture]: Running SimpleCleaner module\n",
      "100%|██████████| 50/50 [00:00<00:00, 287.65it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.60it/s]\n"
     ]
    }
   ],
   "source": [
    "vulture = Vulture(n_jobs=1, verbose=10)\n",
    "df = vulture.clean_dataframe(df=df, \n",
    "                        columns=[\"abstract\", \"title\"],\n",
    "                        append_to_original_df=True,\n",
    "                        concat_cleaned_cols=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     vulnerabilities pose threat cybersecurity kern...\n",
       "1     cybersecurity frameworks nist guidelines risk ...\n",
       "2     bias-variance tradeoff determines models abili...\n",
       "3     efficient distributed implementation truncated...\n",
       "4     autoencoders dimensionality reduction anomaly ...\n",
       "5     ensemble bagging boosting combine multiple mod...\n",
       "6     vulnerabilities pose threat cybersecurity gene...\n",
       "7     preprocessing steps normalization handling mis...\n",
       "8     feature engineering plays crucial role improvi...\n",
       "9     federated learning ai models trained decentral...\n",
       "10    principal analysis pca reducing dimensionality...\n",
       "11    malware dangerous costly cyber threats nationa...\n",
       "12    cybersecurity frameworks nist guidelines risk ...\n",
       "13    transfer learning pre-trained models fine-tune...\n",
       "14    autoencoders effectively unsupervised feature ...\n",
       "15    identification family malware specimen belongs...\n",
       "16    ensemble bagging boosting combine multiple mod...\n",
       "17    blockchain technology enhances security transp...\n",
       "18    malware dangerous costly cyber threats nationa...\n",
       "19    adversarial machine learning explores enhance ...\n",
       "20    efficient distributed implementation matrix fa...\n",
       "21    malware dangerous costly cyber threats nationa...\n",
       "22    deep reinforcement learning achieved breakthro...\n",
       "23    feature engineering plays crucial role improvi...\n",
       "24    cybersecurity frameworks nist guidelines risk ...\n",
       "25    curse dimensionality machine learning models t...\n",
       "26    matrix factorization nmf completion collaborat...\n",
       "27    highly specific scientific literature educatio...\n",
       "28    vulnerabilities pose threat cybersecurity kern...\n",
       "29    deep reinforcement learning achieved breakthro...\n",
       "30    adversarial machine learning explores enhance ...\n",
       "31    transfer learning pre-trained models fine-tune...\n",
       "32    cybersecurity frameworks nist guidelines risk ...\n",
       "33    highly specific scientific literature educatio...\n",
       "34    cybersecurity frameworks nist guidelines risk ...\n",
       "35    gradient descent essential optimization algori...\n",
       "36    anomaly techniques fraud cybersecurity supervi...\n",
       "37    autoencoders effectively unsupervised feature ...\n",
       "38    kernel trick svms enables efficient classifica...\n",
       "39    explainable ai xai enhances trust ai models ma...\n",
       "40    augmentation techniques improve generalization...\n",
       "41    activation neural networks introduces clusteri...\n",
       "42    overfitting occurs performs training unseen ge...\n",
       "43    curse dimensionality machine learning models g...\n",
       "44    autoencoders dimensionality reduction anomaly ...\n",
       "45    graph neural networks excel processing structu...\n",
       "46    reinforcement learning enables ai agents optim...\n",
       "47    overfitting occurs performs training unseen fe...\n",
       "48    training deep learning models requires computa...\n",
       "49    identification family malware specimen belongs...\n",
       "Name: clean_abstract_title, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.clean_abstract_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build The Vocabulary and the Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'clean_abstract_title'\n",
    "RESULTS = \"result_example\"\n",
    "HIGHLIGHT_WORDS = ['analysis', 'tensor']\n",
    "HIGHLIGHT_WEIGHTS = [2 for i in HIGHLIGHT_WORDS]\n",
    "beaver = Beaver()\n",
    "os.makedirs(RESULTS, exist_ok=True)\n",
    "settings = {\n",
    "    \"dataset\" : df,\n",
    "    \"target_column\" : DATA_COLUMN,\n",
    "    'highlighting': HIGHLIGHT_WORDS,\n",
    "    'weights':HIGHLIGHT_WEIGHTS,\n",
    "    \"matrix_type\" : \"tfidf\",\n",
    "    \"save_path\" : RESULTS\n",
    "}\n",
    "X, vocabulary = beaver.documents_words(**settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float32'\n",
       "\twith 4968 stored elements and shape (607, 50)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.T.tocsr()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape[1] == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abstaining', 'accelerated', 'accuracy', 'accurate', 'achieved',\n",
       "       'acquisition', 'activation', 'activity', 'addition', 'additional'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorize with HNMFk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of cluster numbers (K) to search over\n",
    "Ks = np.arange(2, 10, 1)  # From 2 to 29 inclusive\n",
    "\n",
    "# Number of perturbations and iterations to run\n",
    "perts = 2  # Number of perturbed runs to estimate stability\n",
    "iters = 2  # Number of iterations for each perturbation\n",
    "\n",
    "# Small perturbation epsilon added to input data\n",
    "eps = 0.025\n",
    "\n",
    "# Initialization method for NMF (Non-negative Matrix Factorization)\n",
    "init = \"nnsvd\"  # Nonnegative SVD initialization\n",
    "\n",
    "# Path to save HNMFk results\n",
    "HNMFK_save_path = os.path.join(RESULTS, \"example_HNMFK\")\n",
    "name = HNMFK_save_path  # Alias for convenience\n",
    "\n",
    "# Parameters for HNMFk (Hierarchical Nonnegative Matrix Factorization k-search)\n",
    "nmfk_params = {\n",
    "    \"k_search_method\": \"bst_pre\",             # Method for determining optimal k (e.g., binary search with pre-checks)\n",
    "    \"sill_thresh\": 0.7,                       # Silhouette threshold to accept a given k\n",
    "    \"H_sill_thresh\": 0.05,                    # Threshold for H-matrix silhouette to refine k selection\n",
    "    \"n_perturbs\": perts,                      # Number of perturbations\n",
    "    \"n_iters\": iters,                         # Number of iterations per perturbation\n",
    "    \"epsilon\": eps,                           # Perturbation strength\n",
    "    \"n_jobs\": -1,                             # Use all available CPU cores\n",
    "    \"init\": init,                             # NMF initialization method\n",
    "    \"use_gpu\": False,                         # Whether to use GPU acceleration\n",
    "    \"save_path\": HNMFK_save_path,             # Directory where results will be saved\n",
    "    \"predict_k_method\": \"WH_sill\",            # Method to predict k using W and H matrix silhouettes\n",
    "    \"predict_k\": True,                        # Whether to automatically predict k\n",
    "    \"verbose\": False,                          # Verbose output\n",
    "    \"nmf_verbose\": False,                     # Verbose output from NMF algorithm\n",
    "    \"transpose\": False,                       # Whether to transpose input data\n",
    "    \"pruned\": True,                           # Whether to prune unstable clusters\n",
    "    \"nmf_method\": \"nmf_fro_mu\",               # NMF solver method (Frobenius norm, multiplicative updates)\n",
    "    \"calculate_error\": False,                 # Whether to calculate reconstruction error\n",
    "    \"use_consensus_stopping\": 0,              # Whether to use consensus stopping (0 = off)\n",
    "    \"calculate_pac\": False,                   # Whether to compute PAC (proportion of ambiguous clustering)\n",
    "    \"consensus_mat\": False,                   # Whether to generate consensus matrix\n",
    "    \"perturb_type\": \"uniform\",                # Type of perturbation (e.g., uniform noise)\n",
    "    \"perturb_multiprocessing\": False,         # Use multiprocessing during perturbation\n",
    "    \"perturb_verbose\": False,                 # Verbose output during perturbation\n",
    "    \"simple_plot\": True                       # Whether to generate simplified summary plots\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSemanticCallback:\n",
    "    def __init__(self, \n",
    "                 df: pd.DataFrame, \n",
    "                 target_column=DATA_COLUMN,\n",
    "                 options={'vocabulary': vocabulary},\n",
    "                 matrix_type=\"tfidf\") -> None:\n",
    "        \"\"\"\n",
    "        Initializes the callback with a DataFrame and matrix generation settings.\n",
    "\n",
    "        Parameters:\n",
    "        - df: The full DataFrame containing the text data.\n",
    "        - target_column: Column name containing the target text to vectorize (default is a global DATA_COLUMN).\n",
    "        - options: Options dictionary passed to Beaver (e.g., fixed vocabulary, token settings).\n",
    "        - matrix_type: Type of vectorization matrix (e.g., \"tfidf\", \"count\").\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.target_column = target_column\n",
    "        self.options = options\n",
    "        self.matrix_type = matrix_type\n",
    "\n",
    "    def __call__(self, original_indices: np.ndarray):\n",
    "        \"\"\"\n",
    "        Callable interface for dynamically generating document-term matrices \n",
    "        from a subset of the DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        - original_indices: Numpy array of row indices from self.df to subset and transform.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple of (X, metadata), where:\n",
    "            - X is a document-term sparse matrix (CSR format).\n",
    "            - metadata is a dict containing either 'vocab' or a 'stop_reason' if failed.\n",
    "        \"\"\"\n",
    "        current_beaver = Beaver()  # Initialize a new instance of the Beaver text vectorizer\n",
    "\n",
    "        # Extract the subset of the DataFrame using the provided indices\n",
    "        current_df = self.df.iloc[original_indices].copy()\n",
    "\n",
    "        # Construct parameters for the Beaver vectorizer\n",
    "        current_beaver_matrix_settings = {\n",
    "            \"dataset\": current_df,\n",
    "            \"target_column\": self.target_column,\n",
    "            \"options\": self.options,\n",
    "            \"highlighting\": HIGHLIGHT_WORDS,     # Global list of words to highlight\n",
    "            \"weights\": HIGHLIGHT_WEIGHTS,        # Associated weights for highlighting\n",
    "            \"matrix_type\": self.matrix_type,     # Type of matrix to construct (e.g., TF-IDF)\n",
    "            \"save_path\": None                    # No file output; matrix is returned\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Attempt to generate the document-word matrix\n",
    "            current_X, vocab = current_beaver.documents_words(**current_beaver_matrix_settings)\n",
    "            \n",
    "            # Transpose to get documents as rows (CSR format is efficient for row slicing)\n",
    "            current_X = current_X.T.tocsr()\n",
    "            \n",
    "            return current_X, {'vocab': vocab}\n",
    "\n",
    "        except:\n",
    "            # On failure, return a 1x1 matrix to signal a stopping condition for downstream tasks\n",
    "            csr_matrix = ss.csr_matrix([[1]])\n",
    "            return csr_matrix, {'stop_reason': \"documents_words couldn't make matrix\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/barron/miniconda3/envs/dev_artic_fox/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/barron/miniconda3/envs/dev_artic_fox/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Parameters for initializing and training the HNMFk model\n",
    "hnmfk_params = {\n",
    "    \"n_nodes\": 1,  # Number of root nodes to begin with (can grow as depth increases)\n",
    "    \n",
    "    # List of NMF parameters for the top-level (depth=0); can use different sets for different nodes\n",
    "    \"nmfk_params\": [nmfk_params],  \n",
    "    \n",
    "    # Callable that generates a document-term matrix from a subset of the DataFrame (dynamic input for each node)\n",
    "    \"generate_X_callback\": CustomSemanticCallback(df=df, options={'vocabulary': vocabulary}),\n",
    "    \n",
    "    \"cluster_on\": \"H\",  # Which factor matrix to use for clustering (H = document-topic)\n",
    "    \n",
    "    \"depth\": 1,  # Depth of the hierarchy; e.g., 2 means root + one layer of children\n",
    "    \n",
    "    \"sample_thresh\": 10,  # Minimum number of samples required to split/cluster a node further\n",
    "    \n",
    "    \"K2\": False,  # If True, forces all subclusters to use k=2; here we allow varying k\n",
    "    \n",
    "    # Range of K to try for deeper layers (children nodes)\n",
    "    \"Ks_deep_min\": 1,\n",
    "    \"Ks_deep_max\": 20,\n",
    "    \"Ks_deep_step\": 1,\n",
    "    \n",
    "    \"experiment_name\": name,  # Folder/identifier for saving results and checkpoints\n",
    "}\n",
    "\n",
    "# Instantiate the HNMFk model with the above parameters\n",
    "model = HNMFk(**hnmfk_params)\n",
    "\n",
    "# Fit the model on matrix X using the specified range of Ks\n",
    "# - from_checkpoint: load previously saved progress if available\n",
    "# - save_checkpoint: periodically save progress for recovery or inspection\n",
    "model.fit(X, Ks, from_checkpoint=False, save_checkpoint=True)\n",
    "\n",
    "# Traverse and collect all nodes created in the hierarchical model\n",
    "all_nodes = model.traverse_nodes()\n",
    "print(len(all_nodes))  # Output the total number of nodes (clusters at all levels)\n",
    "\n",
    "# Save the full trained model to a pickle file for reuse or inspection\n",
    "with open(os.path.join('result_example', 'HNMFK_highlight.pkl'), 'wb') as output_file:\n",
    "    pickle.dump(model, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-process with Arctic Fox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved object state from checkpoint...\n",
      "Step 1: Post-processing W/H matrix and cluster data...\n",
      "Step 2: Labeling clusters with LLM...\n",
      "/projects/SLIC/ryan/telf_internal/examples/ArcticFox/result_example/example_HNMFK/depth_1/Root_0\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 56.98it/s]\n",
      "100%|██████████| 1/1 [00:21<00:00, 21.97s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/SLIC/ryan/telf_internal/examples/ArcticFox/result_example/example_HNMFK/depth_1/Root_1\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 153.63it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/SLIC/ryan/telf_internal/examples/ArcticFox/result_example/example_HNMFK/depth_1/Root_2\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 182.27it/s]\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.41s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/SLIC/ryan/telf_internal/examples/ArcticFox/result_example/example_HNMFK/depth_1/Root_3\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 169.22it/s]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.40s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/SLIC/ryan/telf_internal/examples/ArcticFox/result_example/example_HNMFK/depth_1/Root_4\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 166.37it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.41s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/SLIC/ryan/telf_internal/examples/ArcticFox/result_example/example_HNMFK/depth_1/Root_5\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 130.69it/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.94s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/SLIC/ryan/telf_internal/examples/ArcticFox/result_example/example_HNMFK/depth_1/Root_6\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 149.37it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/SLIC/ryan/telf_internal/examples/ArcticFox/result_example/example_HNMFK/depth_1/Root_7\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 137.13it/s]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.20s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/SLIC/ryan/telf_internal/examples/ArcticFox/result_example/example_HNMFK/depth_1/Root_8\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 141.13it/s]\n",
      "100%|██████████| 12/12 [00:42<00:00,  3.57s/it]\n",
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping cluster_id=0 because it's not in centers.\n",
      "Skipping cluster_id=1 because it's not in centers.\n",
      "Skipping cluster_id=2 because it's not in centers.\n",
      "Skipping cluster_id=3 because it's not in centers.\n",
      "Skipping cluster_id=4 because it's not in centers.\n",
      "Skipping cluster_id=5 because it's not in centers.\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 7/12 [00:01<00:01,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 8/12 [00:02<00:01,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 9/12 [00:04<00:01,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 10/12 [00:05<00:01,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 11/12 [00:07<00:01,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:08<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/SLIC/ryan/telf_internal/examples/ArcticFox/result_example/example_HNMFK/depth_0/Root\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 154.82it/s]\n",
      "100%|██████████| 9/9 [00:25<00:00,  2.88s/it]\n",
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:01<00:13,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:03<00:11,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [00:04<00:09,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:06<00:07,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [00:07<00:06,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:09<00:04,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [00:10<00:02,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [00:12<00:01,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:13<00:00,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Generating Peacock visual stats...\n",
      "\n",
      "10 out of 10 nodes processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained HNMFk model from disk\n",
    "model = HNMFk(experiment_name=os.path.join(\"result_example\", \"example_HNMFK\"))\n",
    "model.load_model()  # Loads model from the provided experiment_name path\n",
    "\n",
    "# Initialize ArcticFox pipeline\n",
    "# - model: the hierarchical clustering model (HNMFk)\n",
    "# - embedding_model: name of the sentence embedding model used for label generation\n",
    "# - clean_cols_name: column in the DataFrame containing the cleaned text input\n",
    "pipeline = ArcticFox(\n",
    "    model=model,\n",
    "    embedding_model=\"SCINCL\",        # Example: SCINCL embedding model fine-tuned for scientific text\n",
    "    clean_cols_name=DATA_COLUMN      # The text column used for label generation and analysis\n",
    ")\n",
    "\n",
    "# Run the full ArcticFox pipeline:\n",
    "# This handles hierarchical cluster labeling, statistics generation, data collection, and label propagation\n",
    "pipeline.run_full_pipeline(\n",
    "    vocab=vocabulary,                # Vocabulary used to guide or filter cluster content\n",
    "    data_df=df,                      # Original dataset (same used in HNMFk)\n",
    "    ollama_model=\"llama3.2:3b-instruct-fp16\",  # Language model used for semantic label generation\n",
    "    label_clusters=True,             # Enable automatic labeling of clusters\n",
    "    generate_stats=True,             # Generate cluster-level statistics\n",
    "    process_parents=True,            # Propagate labels or stats upward through the hierarchy\n",
    "    skip_completed=True,             # Skip processing of nodes already labeled/stored\n",
    "    label_criteria={                 # Rules to filter generated labels\n",
    "        \"minimum words\": 2,\n",
    "        \"maximum words\": 6\n",
    "    },\n",
    "    label_info={                     # Additional metadata to associate with generated labels\n",
    "        \"source\": \"Science\"\n",
    "    },\n",
    "    number_of_labels=5               # Number of candidate labels to generate per node\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_artic_fox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
