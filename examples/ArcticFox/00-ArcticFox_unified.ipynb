{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as ss\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.pre_processing import Vulture\n",
    "from TELF.pre_processing.Vulture.modules import SimpleCleaner\n",
    "from TELF.pre_processing.Vulture.modules import LemmatizeCleaner\n",
    "from TELF.pre_processing.Vulture.modules import RemoveNonEnglishCleaner\n",
    "from TELF.pre_processing.Vulture.default_stop_words import STOP_WORDS\n",
    "from TELF.pre_processing.Vulture.default_stop_phrases import STOP_PHRASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.factorization.HNMFk import HNMFk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.pre_processing import Beaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.post_processing import ArcticFox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"..\", \"..\", \"data\", \"sample2.csv\"))\n",
    "df = df.head(50).reset_index(drop=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [\n",
    "    RemoveNonEnglishCleaner(ascii_ratio=0.9, stopwords_ratio=0.25),\n",
    "    SimpleCleaner(stop_words = STOP_WORDS,\n",
    "                  stop_phrases = STOP_PHRASES,\n",
    "                  order = [\n",
    "                      'standardize_hyphens',\n",
    "                      'isolate_frozen',\n",
    "                      'remove_copyright_statement',\n",
    "                      'remove_stop_phrases',\n",
    "                      'make_lower_case',\n",
    "                      'remove_formulas',\n",
    "                      'normalize',\n",
    "                      'remove_next_line',\n",
    "                      'remove_email',\n",
    "                      'remove_()',\n",
    "                      'remove_[]',\n",
    "                      'remove_special_characters',\n",
    "                      'remove_nonASCII_boundary',\n",
    "                      'remove_nonASCII',\n",
    "                      'remove_tags',\n",
    "                      'remove_stop_words',\n",
    "                      'remove_standalone_numbers',\n",
    "                      'remove_extra_whitespace',\n",
    "                      'min_characters',\n",
    "                  ]\n",
    "                 ),\n",
    "    LemmatizeCleaner('spacy'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vulture = Vulture(n_jobs=1, verbose=10)\n",
    "df = vulture.clean_dataframe(df=df, \n",
    "                        columns=[\"abstract\", \"title\"],\n",
    "                        append_to_original_df=True,\n",
    "                        concat_cleaned_cols=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.clean_abstract_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build The Vocabulary and the Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'clean_abstract_title'\n",
    "RESULTS = \"result_example_unified\"\n",
    "HIGHLIGHT_WORDS = ['analysis', 'tensor']\n",
    "HIGHLIGHT_WEIGHTS = [2 for i in HIGHLIGHT_WORDS]\n",
    "beaver = Beaver()\n",
    "os.makedirs(RESULTS, exist_ok=True)\n",
    "settings = {\n",
    "    \"dataset\" : df,\n",
    "    \"target_column\" : DATA_COLUMN,\n",
    "    'highlighting': HIGHLIGHT_WORDS,\n",
    "    'weights':HIGHLIGHT_WEIGHTS,\n",
    "    \"matrix_type\" : \"tfidf\",\n",
    "    \"save_path\" : RESULTS\n",
    "}\n",
    "X, vocabulary = beaver.documents_words(**settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.T.tocsr()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape[1] == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorize with HNMFk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of cluster numbers (K) to search over\n",
    "Ks = np.arange(2, 10, 1)  # From 2 to 29 inclusive\n",
    "\n",
    "# Number of perturbations and iterations to run\n",
    "perts = 2  # Number of perturbed runs to estimate stability\n",
    "iters = 2  # Number of iterations for each perturbation\n",
    "\n",
    "# Small perturbation epsilon added to input data\n",
    "eps = 0.025\n",
    "\n",
    "# Initialization method for NMF (Non-negative Matrix Factorization)\n",
    "init = \"nnsvd\"  # Nonnegative SVD initialization\n",
    "\n",
    "# Path to save HNMFk results\n",
    "HNMFK_save_path = os.path.join(RESULTS, \"example_HNMFK\")\n",
    "name = HNMFK_save_path  # Alias for convenience\n",
    "\n",
    "# Parameters for HNMFk (Hierarchical Nonnegative Matrix Factorization k-search)\n",
    "nmfk_params = {\n",
    "    \"k_search_method\": \"bst_pre\",             # Method for determining optimal k (e.g., binary search with pre-checks)\n",
    "    \"sill_thresh\": 0.7,                       # Silhouette threshold to accept a given k\n",
    "    \"H_sill_thresh\": 0.05,                    # Threshold for H-matrix silhouette to refine k selection\n",
    "    \"n_perturbs\": perts,                      # Number of perturbations\n",
    "    \"n_iters\": iters,                         # Number of iterations per perturbation\n",
    "    \"epsilon\": eps,                           # Perturbation strength\n",
    "    \"n_jobs\": -1,                             # Use all available CPU cores\n",
    "    \"init\": init,                             # NMF initialization method\n",
    "    \"use_gpu\": False,                         # Whether to use GPU acceleration\n",
    "    \"save_path\": HNMFK_save_path,             # Directory where results will be saved\n",
    "    \"predict_k_method\": \"WH_sill\",            # Method to predict k using W and H matrix silhouettes\n",
    "    \"predict_k\": True,                        # Whether to automatically predict k\n",
    "    \"verbose\": False,                          # Verbose output\n",
    "    \"nmf_verbose\": False,                     # Verbose output from NMF algorithm\n",
    "    \"transpose\": False,                       # Whether to transpose input data\n",
    "    \"pruned\": True,                           # Whether to prune unstable clusters\n",
    "    \"nmf_method\": \"nmf_fro_mu\",               # NMF solver method (Frobenius norm, multiplicative updates)\n",
    "    \"calculate_error\": False,                 # Whether to calculate reconstruction error\n",
    "    \"use_consensus_stopping\": 0,              # Whether to use consensus stopping (0 = off)\n",
    "    \"calculate_pac\": False,                   # Whether to compute PAC (proportion of ambiguous clustering)\n",
    "    \"consensus_mat\": False,                   # Whether to generate consensus matrix\n",
    "    \"perturb_type\": \"uniform\",                # Type of perturbation (e.g., uniform noise)\n",
    "    \"perturb_multiprocessing\": False,         # Use multiprocessing during perturbation\n",
    "    \"perturb_verbose\": False,                 # Verbose output during perturbation\n",
    "    \"simple_plot\": True                       # Whether to generate simplified summary plots\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSemanticCallback:\n",
    "    def __init__(self, \n",
    "                 df: pd.DataFrame, \n",
    "                 target_column=DATA_COLUMN,\n",
    "                 options={'vocabulary': vocabulary},\n",
    "                 matrix_type=\"tfidf\") -> None:\n",
    "        \"\"\"\n",
    "        Initializes the callback with a DataFrame and matrix generation settings.\n",
    "\n",
    "        Parameters:\n",
    "        - df: The full DataFrame containing the text data.\n",
    "        - target_column: Column name containing the target text to vectorize (default is a global DATA_COLUMN).\n",
    "        - options: Options dictionary passed to Beaver (e.g., fixed vocabulary, token settings).\n",
    "        - matrix_type: Type of vectorization matrix (e.g., \"tfidf\", \"count\").\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.target_column = target_column\n",
    "        self.options = options\n",
    "        self.matrix_type = matrix_type\n",
    "\n",
    "    def __call__(self, original_indices: np.ndarray):\n",
    "        \"\"\"\n",
    "        Callable interface for dynamically generating document-term matrices \n",
    "        from a subset of the DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        - original_indices: Numpy array of row indices from self.df to subset and transform.\n",
    "\n",
    "        Returns:\n",
    "        - Tuple of (X, metadata), where:\n",
    "            - X is a document-term sparse matrix (CSR format).\n",
    "            - metadata is a dict containing either 'vocab' or a 'stop_reason' if failed.\n",
    "        \"\"\"\n",
    "        current_beaver = Beaver()  # Initialize a new instance of the Beaver text vectorizer\n",
    "\n",
    "        # Extract the subset of the DataFrame using the provided indices\n",
    "        current_df = self.df.iloc[original_indices].copy()\n",
    "\n",
    "        # Construct parameters for the Beaver vectorizer\n",
    "        current_beaver_matrix_settings = {\n",
    "            \"dataset\": current_df,\n",
    "            \"target_column\": self.target_column,\n",
    "            \"options\": self.options,\n",
    "            \"highlighting\": HIGHLIGHT_WORDS,     # Global list of words to highlight\n",
    "            \"weights\": HIGHLIGHT_WEIGHTS,        # Associated weights for highlighting\n",
    "            \"matrix_type\": self.matrix_type,     # Type of matrix to construct (e.g., TF-IDF)\n",
    "            \"save_path\": None                    # No file output; matrix is returned\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Attempt to generate the document-word matrix\n",
    "            current_X, vocab = current_beaver.documents_words(**current_beaver_matrix_settings)\n",
    "            \n",
    "            # Transpose to get documents as rows (CSR format is efficient for row slicing)\n",
    "            current_X = current_X.T.tocsr()\n",
    "            \n",
    "            return current_X, {'vocab': vocab}\n",
    "\n",
    "        except:\n",
    "            # On failure, return a 1x1 matrix to signal a stopping condition for downstream tasks\n",
    "            csr_matrix = ss.csr_matrix([[1]])\n",
    "            return csr_matrix, {'stop_reason': \"documents_words couldn't make matrix\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for initializing and training the HNMFk model\n",
    "hnmfk_params = {\n",
    "    \"n_nodes\": 1,  # Number of root nodes to begin with (can grow as depth increases)\n",
    "    \n",
    "    # List of NMF parameters for the top-level (depth=0); can use different sets for different nodes\n",
    "    \"nmfk_params\": [nmfk_params],  \n",
    "    \n",
    "    # Callable that generates a document-term matrix from a subset of the DataFrame (dynamic input for each node)\n",
    "    \"generate_X_callback\": CustomSemanticCallback(df=df, options={'vocabulary': vocabulary}),\n",
    "    \n",
    "    \"cluster_on\": \"H\",  # Which factor matrix to use for clustering (H = document-topic)\n",
    "    \n",
    "    \"depth\": 1,  # Depth of the hierarchy; e.g., 2 means root + one layer of children\n",
    "    \n",
    "    \"sample_thresh\": 10,  # Minimum number of samples required to split/cluster a node further\n",
    "    \n",
    "    \"K2\": False,  # If True, forces all subclusters to use k=2; here we allow varying k\n",
    "    \n",
    "    # Range of K to try for deeper layers (children nodes)\n",
    "    \"Ks_deep_min\": 1,\n",
    "    \"Ks_deep_max\": 20,\n",
    "    \"Ks_deep_step\": 1,\n",
    "    \n",
    "    \"experiment_name\": name,  # Folder/identifier for saving results and checkpoints\n",
    "}\n",
    "\n",
    "# Instantiate the HNMFk model with the above parameters\n",
    "model = HNMFk(**hnmfk_params)\n",
    "\n",
    "# Fit the model on matrix X using the specified range of Ks\n",
    "# - from_checkpoint: load previously saved progress if available\n",
    "# - save_checkpoint: periodically save progress for recovery or inspection\n",
    "model.fit(X, Ks, from_checkpoint=False, save_checkpoint=True)\n",
    "\n",
    "# Traverse and collect all nodes created in the hierarchical model\n",
    "all_nodes = model.traverse_nodes()\n",
    "print(len(all_nodes))  # Output the total number of nodes (clusters at all levels)\n",
    "\n",
    "# Save the full trained model to a pickle file for reuse or inspection\n",
    "with open(os.path.join(RESULTS, 'HNMFK_highlight.pkl'), 'wb') as output_file:\n",
    "    pickle.dump(model, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-process with Arctic Fox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.post_processing.ArcticFox import ClusteringAnalyzer\n",
    "\n",
    "hnmfk_analyzer = ClusteringAnalyzer(\n",
    "    top_n_words=2,\n",
    "    out_dir=\"example_out/hnmfk\",\n",
    "    archive_subdir=\"archive_hnmfk\"\n",
    ")\n",
    "out4 = hnmfk_analyzer.analyze(\n",
    "    df=df,\n",
    "    hnmfk_model=model,\n",
    "    vocab=vocabulary,\n",
    "    clean_cols_name='clean_abstract_title',\n",
    "    process_parents=True,\n",
    "    skip_completed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - stats, peacock, wolf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_artic_fox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
