{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeec913a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results directory exist.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import os; os.mkdir(\"results\")\n",
    "except:\n",
    "    print(\"results directory exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba32e08-fd2e-495c-9a5d-167af1cd5a06",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "345d5d8c-bbd5-410e-9592-95d5f93f1c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle \n",
    "\n",
    "documents = pickle.load(open(\"../../data/documents.p\", \"rb\"))\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d76a0f09-cdcf-4cf2-ac54-1362136e1736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ad68055e-677f-11ee-95d4-4ab2673ea3f0', 'ad680626-677f-11ee-95d4-4ab2673ea3f0', 'ad680658-677f-11ee-95d4-4ab2673ea3f0', 'ad680680-677f-11ee-95d4-4ab2673ea3f0', 'ad6806a8-677f-11ee-95d4-4ab2673ea3f0', 'ad6806d0-677f-11ee-95d4-4ab2673ea3f0', 'ad6806f8-677f-11ee-95d4-4ab2673ea3f0', 'ad680716-677f-11ee-95d4-4ab2673ea3f0', 'ad68073e-677f-11ee-95d4-4ab2673ea3f0'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2f2f25a-c3ad-40b1-bba1-652833367173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Supervisory Control and Data Acquisition (SCADA) systems often serve as the nervous system for substations within power grids. These systems facilitate real-time monitoring, data acquisition, control of equipment, and ensure smooth and efficient operation of the substation and its connected devices. As the dependence on these SCADA systems grows, so does the risk of potential malicious intrusions that could lead to significant outages or even permanent damage to the grid. Previous work has shown that dimensionality reduction-based approaches, such as Principal Component Analysis (PCA), can be used for accurate identification of anomalies in SCADA systems. While not specifically applied to SCADA, non-negative matrix factorization (NMF) has shown strong results at detecting anomalies in wireless sensor networks. These unsupervised approaches model the normal or expected behavior and detect the unseen types of attacks or anomalies by identifying the events that deviate from the expected behavior. These approaches; however, do not model the complex and multi-dimensional interactions that are naturally present in SCADA systems. Differently, non-negative tensor decomposition is a powerful unsupervised machine learning (ML) method that can model the complex and multi-faceted activity details of SCADA events. In this work, we novelly apply the tensor decomposition method Canonical Polyadic Alternating Poisson Regression (CP-APR) with a probabilistic framework, which has previously shown state-of-the-art anomaly detection results on cyber network data, to identify anomalies in SCADA systems. We showcase that the use of statistical behavior analysis of SCADA communication with tensor decomposition improves the specificity and accuracy of identifying anomalies in electrical grid systems. In our experiments, we model real-world SCADA system data collected from the electrical grid operated by Los Alamos National Laboratory (LANL) which provides transmission and distribution service through a partnership with Los Alamos County, and detect synthetically generated anomalies.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[list(documents.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96f0ce52-c75a-4d19-bf18-501b2a4f0681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"../../data/stop_words.txt\", \"r\")\n",
    "stop_words = file.read().split(\"\\n\")\n",
    "file.close()\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a708c17b-f4ad-480c-b31f-f221e034a981",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8718d66-6e24-408f-ab51-d3f5e8f01cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.pre_processing import Vulture\n",
    "\n",
    "settings = {\n",
    "    # number of parallel jobs\n",
    "    \"n_jobs\":-1, \n",
    "    # number of nodes\n",
    "    \"n_nodes\":1, \n",
    "    # verbosity level\n",
    "    \"verbose\":1,\n",
    "    # minimum number of characters in tokens\n",
    "    \"min_characters\":2,\n",
    "    # minimum number of unique characters in tokens\n",
    "    \"min_unique_characters\":2,\n",
    "    # Lemmatize the tokens\n",
    "    \"lemmatize\":True,\n",
    "    # Lemmatize document using Spacy, performed during advance pre-processing\n",
    "    \"lemmatize_spacy\":True,\n",
    "    # nltk stems and substitution\n",
    "    'lemmatize_slic':True,\n",
    "    # Stemming of the tokens\n",
    "    \"stem\":False,\n",
    "    # Perform text cleaning\n",
    "    \"clean_text\":True,\n",
    "    # If True, advance text cleaning, otherwise simple only\n",
    "    \"advance_clean\":True,\n",
    "    # Allowed postags for Spacy, used if advance_clean=True\n",
    "    \"allowed_postags\":['NOUN', 'ADJ', 'VERB', 'ADV', \"PROPN\"],\n",
    "    # Spacy NLP model, used if advance_clean=True\n",
    "    \"spacy_model\":\"en_core_web_lg\",\n",
    "    # If True, detects language\n",
    "    \"detect_language\":True,\n",
    "    # Number of words to use when detecting language, used if detect_language=True\n",
    "    \"n_words_use_language\":10,\n",
    "    # backend to use\n",
    "    \"parallel_backend\":\"loky\",\n",
    "    # Settings for simple cleaning\n",
    "    \"simple_clean_settings\": { \"remove_copyright_with_symbol\": True,\n",
    "                               \"remove_stop_phrases\": False,\n",
    "                               \"make_lower_case\": True,\n",
    "                               \"remove_trailing_dash\": True,  # -foo-bar- -> foo-bar\n",
    "                               \"make_hyphens_words\": False,  # foo-bar -> foobar\n",
    "                               \"remove_next_line\": True,\n",
    "                               \"remove_email\": True,\n",
    "                               \"remove_dash\": False,\n",
    "                               \"remove_between_[]\": True,\n",
    "                               \"remove_between_()\": True,\n",
    "                               \"remove_[]\": False,\n",
    "                               \"remove_()\": False,\n",
    "                               \"remove_\\\\\": False,\n",
    "                               \"remove_^\": False,\n",
    "                               \"remove_numbers\": False,\n",
    "                               \"remove_nonASCII\": False,\n",
    "                               \"remove_tags\": True,  # remove HTML tags\n",
    "                               \"remove_special_characters\": True,  # option to specify these?\n",
    "                               \"remove_stop_words\": True,\n",
    "                            },\n",
    "}\n",
    "vulture = Vulture(**settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c156944-27c0-4580-8052-14a0622e5c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "substitutions = {'pair':'pair_modded','theory':'moddified_theories',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e455de19-67aa-45f8-b1e1-aaa4e58714d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    4.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories 'results' already exists.\n",
      "Performing language detection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing non-english documents through heuristic.\n",
      "Removed 0 non-english documents.\n",
      "Performing simple-preprocess.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing advance pre-process with Spacy lemmatization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    4.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing lemattization.\n",
      "Performing character length checks.\n",
      "Performing advance pre-process with Slic lemmatization (nltk stems).\n",
      "Directories 'results' already exists.\n",
      "Token length: 1196, First ten tokens before slic lemma: ['supervisory', 'control', 'acquisition', 'system', 'often', 'serve', 'nervous', 'substation', 'power', 'grid']\n",
      "Directories 'results' already exists.\n",
      "Done. Time= 14.591508865356445\n",
      "CPU times: user 131 ms, sys: 169 ms, total: 300 ms\n",
      "Wall time: 14.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    2.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "%time vulture.clean(documents, stop_words, filename=\"results/clean_example\", substitutions=substitutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96259b6-c967-4102-913a-9e866e4ee111",
   "metadata": {},
   "source": [
    "# Look at Cleaned Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aad256c-2645-4f1e-b98a-f44132e61c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_documents = pickle.load(open(\"results/clean_example.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3fb3cae-24d7-4c8c-90e0-7ad2dc9e1604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ad68055e-677f-11ee-95d4-4ab2673ea3f0', 'ad680626-677f-11ee-95d4-4ab2673ea3f0', 'ad680658-677f-11ee-95d4-4ab2673ea3f0', 'ad680680-677f-11ee-95d4-4ab2673ea3f0', 'ad6806a8-677f-11ee-95d4-4ab2673ea3f0', 'ad6806d0-677f-11ee-95d4-4ab2673ea3f0', 'ad6806f8-677f-11ee-95d4-4ab2673ea3f0', 'ad680716-677f-11ee-95d4-4ab2673ea3f0', 'ad68073e-677f-11ee-95d4-4ab2673ea3f0'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_documents.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d3eb8c4-504b-4830-b354-a027e6e8ebed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'supervisory control acquisition system often serve nervous substation power grid system facilitate real time monitor acquisition control equipment ensure smooth efficient operation substation connect device dependence scada system grow risk potential malicious intrusion significant outage even permanent damage grid dimensionality reduction base approach principal analysis accurate identification anomaly scada system specific scada negative matrix factorization strong detect anomaly wireless sensor network unsupervised approach normal expect behavior detect unseen type attack anomaly identify event deviate expect behavior approach complex multi dimensionality interaction naturally scada system differently negative tensor decomposition power unsupervised machine learn complex multi faceted activity detail scada event novelly apply tensor decomposition canonical polyadic alternate poisson regression probabilistic framework previously state art anomaly cyber network identify anomaly scada system showcase statistical behavior analysis scada communication tensor decomposition improve specific accuracy identify anomaly electrical grid system experiment real world scada collective electrical grid operation los alamos national laboratory provide transmission distribute service partnership los alamos county detect synthetically generate anomaly'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Clean:\")\n",
    "clean_documents[list(documents.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31018d2a-9d27-4e6e-82b0-09bd7cbe93db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Supervisory Control and Data Acquisition (SCADA) systems often serve as the nervous system for substations within power grids. These systems facilitate real-time monitoring, data acquisition, control of equipment, and ensure smooth and efficient operation of the substation and its connected devices. As the dependence on these SCADA systems grows, so does the risk of potential malicious intrusions that could lead to significant outages or even permanent damage to the grid. Previous work has shown that dimensionality reduction-based approaches, such as Principal Component Analysis (PCA), can be used for accurate identification of anomalies in SCADA systems. While not specifically applied to SCADA, non-negative matrix factorization (NMF) has shown strong results at detecting anomalies in wireless sensor networks. These unsupervised approaches model the normal or expected behavior and detect the unseen types of attacks or anomalies by identifying the events that deviate from the expected behavior. These approaches; however, do not model the complex and multi-dimensional interactions that are naturally present in SCADA systems. Differently, non-negative tensor decomposition is a powerful unsupervised machine learning (ML) method that can model the complex and multi-faceted activity details of SCADA events. In this work, we novelly apply the tensor decomposition method Canonical Polyadic Alternating Poisson Regression (CP-APR) with a probabilistic framework, which has previously shown state-of-the-art anomaly detection results on cyber network data, to identify anomalies in SCADA systems. We showcase that the use of statistical behavior analysis of SCADA communication with tensor decomposition improves the specificity and accuracy of identifying anomalies in electrical grid systems. In our experiments, we model real-world SCADA system data collected from the electrical grid operated by Los Alamos National Laboratory (LANL) which provides transmission and distribution service through a partnership with Los Alamos County, and detect synthetically generated anomalies.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Raw:\")\n",
    "documents[list(documents.keys())[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee179741-2172-4a56-931f-14d1109a44a4",
   "metadata": {},
   "source": [
    "# Look at Top Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20108ef9-3c43-45d0-a078-3dcd6a5cb2e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ad68055e-677f-11ee-95d4-4ab2673ea3f0': 'supervisory control acquisition system often serve nervous substation power grid system facilitate real time monitor acquisition control equipment ensure smooth efficient operation substation connect device dependence scada system grow risk potential malicious intrusion significant outage even permanent damage grid dimensionality reduction base approach principal analysis accurate identification anomaly scada system specific scada negative matrix factorization strong detect anomaly wireless sensor network unsupervised approach normal expect behavior detect unseen type attack anomaly identify event deviate expect behavior approach complex multi dimensionality interaction naturally scada system differently negative tensor decomposition power unsupervised machine learn complex multi faceted activity detail scada event novelly apply tensor decomposition canonical polyadic alternate poisson regression probabilistic framework previously state art anomaly cyber network identify anomaly scada system showcase statistical behavior analysis scada communication tensor decomposition improve specific accuracy identify anomaly electrical grid system experiment real world scada collective electrical grid operation los alamos national laboratory provide transmission distribute service partnership los alamos county detect synthetically generate anomaly',\n",
       " 'ad680626-677f-11ee-95d4-4ab2673ea3f0': 'highly specific dataset scientific literature research education difficult build dataset scale common build dataset reduction apply topic model establish corpus selection specific topic robust time consume build dataset constructively subject matter expert handpick document scale prone error dataset grow showcase tool machine learn constructively generate target dataset scientific literature initial corpus paper build citation network document citation network generate text embed visualize embed dimensionality reduction paper keep dataset similar core otherwise prune human loop selection addition insight paper gain sub topic model senmfk tool literature review apply field machine learn',\n",
       " 'ad680658-677f-11ee-95d4-4ab2673ea3f0': 'propose efficient distribute memory implementation negative matrix factorization heterogeneous high perform compute system implementation prior nmfk perform automatic selection extract latent variable pattern extend nmfk support dense sparse matrix operation multi node multi gpu system result optimize memory problem memory factorization matrix greater available gpu memory memory complex reduce batch tile strategy sparse dense matrix operation significantly accelerate gpu core latency associate batch copy host device hide cuda stream overlap transfer compute asynchronously latency associate collective communication reduce optimize nvidia collective communication library communication benchmark significant improve 32x speedup implementation gpus cpu base nmfk good weak scale demonstrate multi gpu cluster node approximately gpu decompose dense terabyte size matrix exabyte size sparse matrix density',\n",
       " 'ad680680-677f-11ee-95d4-4ab2673ea3f0': 'identification family malware specimen belong essential understand behavior malware develop mitigation strategy solution prior often practicality lack realistic evaluation factorization factorization learn class imbalance ability identify malware cost production quality label practicality deploy model face prominent rare malware family obtain quantity date label malware train expensive address problem propose novel hierarchical semi supervise call hnmfk classify early stage malware family label negative matrix factorization automatic selection estimation cluster hnmfk classify exploit hierarchical structure malware together semi supervise setup enable classify malware family condition extreme class imbalance perform abstain prediction rejection option yield identification novel malware family help maintain perform quantity label perform bulk classification nearly rare prominent malware family static analysis nearly sample ember-2018 corpus experiment surpass supervise semi supervise baseline model score',\n",
       " 'ad6806a8-677f-11ee-95d4-4ab2673ea3f0': 'malware dangerous costly cyber threat national security crucial factorization modern cyber space adoption machine learn solution malware threat relatively shortcoming exist approach likely contribute majority current approach ignore real world challenge novel malware addition approach often design malware benign ware classification malware family classification showcase preliminary capability perform precise identification novel malware family unify capability malware benign ware classification malware family classification framework',\n",
       " 'ad6806d0-677f-11ee-95d4-4ab2673ea3f0': 'malware dangerous costly cyber threat organization public national security crucial factorization modern warfare adoption base solution malware threat relatively potential cost save majority malware defense solution sufficiently address follow real world challenge consider cost associate label malware supervise solution poorly generate malware train test model class imbalance rare prominent malware include incorporate ability identify novel malware family cybersecurity analyst regularly quantity malware sample understand specimen belong previously malware family classify malware sample family reduce file analyst need examine aid understand behavior malware help estimation severity threat develop mitigation strategy reduce cost spend malware analysis develop name malware dna malware family classification characterization identification state art address majority shortcoming field malware dna consider malware analogous genome dna explore hide hierarchical structure malware prior idea smarttensor platform d100 winner category recognize d100 market disruptor bronze medal enable discovery multi structure composition malware separate mix latent feature hierarchical explore semi supervise unsupervised technique yield generate malware dna analogous innovation take follow idea recently method human cancer mutation genome cause inherit disease certain cancer similarly project treat malware malicious mutation software genome target extract recognition mutation malware signature hierarchical tensor factorization incorporate automatic determination combine ability perform abstain prediction allow malware dna identify classify rare prominent malware label imbalance maintain accuracy even low quantity label detect novel malware build archive latent multi modal signature combine describe characterization complex archive rapid real time characterization classification detect unknown novel phenomenon preliminary study create catalog malware multiplication family benign specimen static analysis feature ember-2018 dataset6 malware class novel family showcase perform classify malware family benign ware detect specimen belong novel malware family report curve risk coverage score',\n",
       " 'ad6806f8-677f-11ee-95d4-4ab2673ea3f0': 'topic model key analytic technique organization analysis text corpora topic model recently introduce senmfk semantic negative matrix factorization automatic determination text document matrix word context matrix jointly factorization text document matrix frequency inverse document frequency matrix word context matrix represent time word occur pre determination window text incorporate semantic structure text ability topic enable coherent separate latent topic accurate document cluster identify high topic topic theme text corpus often complex structure sub topic theme set document separate topic sport politic science topic separate sub topic sport theme separate subtopic tennis soccer football repeat expand separate find sub topic corpus hierarchical senmfk extract fine grain sub topic semantic sub structure hierarchical apply senmfk break topic extract previously unknown sub topic correspond sub semantic structure serve narrow vocabulary scientific jargon seed name entity recognition hierarchical senmfk perform topic model paper post arxiv paper enhance semantic cluster topic jointly factorization category text matrix represent idf token document category category pre determination report author document field research arxiv ability practicality hierarchical senmfk extract meaningful topic semantic sub structure dataset',\n",
       " 'ad680716-677f-11ee-95d4-4ab2673ea3f0': 'negative matrix factorization miss value completion well know collaborative filter personalize user recommender traditional rely privacy invasive collective user build central recommender shoot federate learn recently emerge mitigation privacy address traditional communication bottleneck federate learn shoot federate implementation name fedcf group user collaborative organization client apply parallel build distinct client specific recommender privacy preserve item pattern bias client share processor perform joint factorization extract item pattern extract pattern aggregate client build model information retrieval transfer experiment movielen dataset competitive state art federate recommender system substantial communication',\n",
       " 'ad68073e-677f-11ee-95d4-4ab2673ea3f0': 'propose efficient distribute memory implementation truncate singular decomposition heterogeneous perform compute system implementation svd singular estimation singular vector significantly memory complex propose implementation svd power truncate singular singular vector estimation memory utilization bottleneck power typically associate compute gram matrix significant dense super large sparse implementation optimize memory problem memory factorization matrix greater available gpu memory reduce memory complex batch strategy intermediate factorization compute block block suppress latency associate host device device host batch copy overlap batch copy compute cuda stream furthermore optimize communication reduce latency associate collective communication addition sparse dense matrix multiplication significantly accelerate gpu core result implementation good scale scalability distribute core svd successfully decompose dense matrix size sparse matrix size 128pb sparsity'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a6c007b-32e2-460a-9410-eabc50d1d43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 2092.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tf</th>\n",
       "      <th>df</th>\n",
       "      <th>df_fraction</th>\n",
       "      <th>tf_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>malware</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.097252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>matrix</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.044397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>topic</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.042283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>family</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.035941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>factorization</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.033827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>furthermore</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>scalability</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>successfully</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>128pb</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>sparsity</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.002114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>473 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  tf  df  df_fraction  tf_fraction\n",
       "0          malware  46   3     0.333333     0.097252\n",
       "1           matrix  21   6     0.666667     0.044397\n",
       "2            topic  20   2     0.222222     0.042283\n",
       "3           family  17   3     0.333333     0.035941\n",
       "4    factorization  16   8     0.888889     0.033827\n",
       "..             ...  ..  ..          ...          ...\n",
       "468    furthermore   1   1     0.111111     0.002114\n",
       "469    scalability   1   1     0.111111     0.002114\n",
       "470   successfully   1   1     0.111111     0.002114\n",
       "471          128pb   1   1     0.111111     0.002114\n",
       "472       sparsity   1   1     0.111111     0.002114\n",
       "\n",
       "[473 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from TELF.pre_processing.Vulture.tokens_analysis.top_words import get_top_words\n",
    "\n",
    "top_words_df = get_top_words(clean_documents, \n",
    "                             top_n=99999999, \n",
    "                             n_gram=1, \n",
    "                             verbose=True,\n",
    "                             filename=None)\n",
    "top_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "190f66fe-7253-4b4d-acce-2127e83b2a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 6474.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tf</th>\n",
       "      <th>df</th>\n",
       "      <th>df_fraction</th>\n",
       "      <th>tf_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>malware family</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.012922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub topic</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.005964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>novel malware</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.005964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative matrix</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.004970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>matrix factorization</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.004970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>topic model</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.004970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>scada system</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.003976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>machine learn</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.003976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>latency associate</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.003976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>semi supervise</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.003976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   word  tf  df  df_fraction  tf_fraction\n",
       "0        malware family  13   3     0.333333     0.012922\n",
       "1             sub topic   6   2     0.222222     0.005964\n",
       "2         novel malware   6   3     0.333333     0.005964\n",
       "3       negative matrix   5   5     0.555556     0.004970\n",
       "4  matrix factorization   5   5     0.555556     0.004970\n",
       "5           topic model   5   2     0.222222     0.004970\n",
       "6          scada system   4   1     0.111111     0.003976\n",
       "7         machine learn   4   3     0.333333     0.003976\n",
       "8     latency associate   4   2     0.222222     0.003976\n",
       "9        semi supervise   4   2     0.222222     0.003976"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_2grams_df = get_top_words(clean_documents, \n",
    "                             top_n=10, \n",
    "                             n_gram=2, \n",
    "                             verbose=True,\n",
    "                             filename=None,)\n",
    "top_2grams_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab98de8f-427f-4f8a-a923-ab5f65533ae8",
   "metadata": {},
   "source": [
    "# Look at Slic lemma stem mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "755751f4-5068-44df-a718-5f268814cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "clean_documents = pd.read_csv(\"results/vocabulary_stem_map.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb9c8a75-11d7-45fe-9667-4c772eb7b6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>systems</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>powerful</td>\n",
       "      <td>power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>operate</td>\n",
       "      <td>operation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dimensional</td>\n",
       "      <td>dimensionality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reductively</td>\n",
       "      <td>reduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>specifically</td>\n",
       "      <td>specific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>specificity</td>\n",
       "      <td>specific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>factorize</td>\n",
       "      <td>factorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>factor</td>\n",
       "      <td>factorization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unsupervise</td>\n",
       "      <td>unsupervised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>complexity</td>\n",
       "      <td>complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>communicator</td>\n",
       "      <td>communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>improvement</td>\n",
       "      <td>improve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>collect</td>\n",
       "      <td>collective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>collection</td>\n",
       "      <td>collective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>distribution</td>\n",
       "      <td>distribute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>generalize</td>\n",
       "      <td>generate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>generalization</td>\n",
       "      <td>generate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>performance</td>\n",
       "      <td>perform</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>computation</td>\n",
       "      <td>compute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>select</td>\n",
       "      <td>selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>extraction</td>\n",
       "      <td>extract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>major</td>\n",
       "      <td>majority</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>additional</td>\n",
       "      <td>addition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>topics</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>organize</td>\n",
       "      <td>organization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>recent</td>\n",
       "      <td>recently</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>determine</td>\n",
       "      <td>determination</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>separation</td>\n",
       "      <td>separate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>hierarchically</td>\n",
       "      <td>hierarchical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>practicable</td>\n",
       "      <td>practicality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>practice</td>\n",
       "      <td>practicality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>collaborate</td>\n",
       "      <td>collaborative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>recommendation</td>\n",
       "      <td>recommender</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>mitigate</td>\n",
       "      <td>mitigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>estimate</td>\n",
       "      <td>estimation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>multiple</td>\n",
       "      <td>multiplication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>classifier</td>\n",
       "      <td>classify</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>helpful</td>\n",
       "      <td>help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>characterize</td>\n",
       "      <td>characterization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>analogy</td>\n",
       "      <td>analogous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>genomic</td>\n",
       "      <td>genome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>exploration</td>\n",
       "      <td>explore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>mutational</td>\n",
       "      <td>mutation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>combination</td>\n",
       "      <td>combine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0                 1\n",
       "0          systems            system\n",
       "1         powerful             power\n",
       "2          operate         operation\n",
       "3      dimensional    dimensionality\n",
       "4      reductively         reduction\n",
       "5     specifically          specific\n",
       "6      specificity          specific\n",
       "7        factorize     factorization\n",
       "8           factor     factorization\n",
       "9      unsupervise      unsupervised\n",
       "10      complexity           complex\n",
       "11    communicator     communication\n",
       "12     improvement           improve\n",
       "13         collect        collective\n",
       "14      collection        collective\n",
       "15    distribution        distribute\n",
       "16      generalize          generate\n",
       "17  generalization          generate\n",
       "18     performance           perform\n",
       "19     computation           compute\n",
       "20          select         selection\n",
       "21      extraction           extract\n",
       "22           major          majority\n",
       "23      additional          addition\n",
       "24          topics             topic\n",
       "25        organize      organization\n",
       "26          recent          recently\n",
       "27       determine     determination\n",
       "28      separation          separate\n",
       "29  hierarchically      hierarchical\n",
       "30     practicable      practicality\n",
       "31        practice      practicality\n",
       "32     collaborate     collaborative\n",
       "33  recommendation       recommender\n",
       "34        mitigate        mitigation\n",
       "35        estimate        estimation\n",
       "36        multiple    multiplication\n",
       "37      classifier          classify\n",
       "38         helpful              help\n",
       "39    characterize  characterization\n",
       "40         analogy         analogous\n",
       "41         genomic            genome\n",
       "42     exploration           explore\n",
       "43      mutational          mutation\n",
       "44     combination           combine"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5027d5bf-a68e-4ece-9273-7f7ee3e5e0dc",
   "metadata": {},
   "source": [
    "# look at size differences in corpus tokens before and after cleaning (only saves in slic lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0b1f055-10d2-4ebf-b60d-94efb71104ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('results/before_any_cleaning-vocabulary.txt') as f:\n",
    "    pre_clean = f.read().splitlines() \n",
    "with open('results/after_slic_lemma-vocabulary.txt') as f:\n",
    "    post_clean = f.read().splitlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15c4e4de-58a0-45e8-a79e-e5e5ac91db24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_len: 2334 | post_len: 1196 | difference= 1138\n"
     ]
    }
   ],
   "source": [
    "pre_len = len(pre_clean)\n",
    "post_len = len(post_clean)\n",
    "\n",
    "print(f'pre_len: {pre_len} | post_len: {post_len} | difference= {pre_len-post_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b452a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
