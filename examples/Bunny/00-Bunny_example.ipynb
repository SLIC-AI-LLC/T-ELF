{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note that this notebook is a demonstration of how to use Bunny. Since we are using mock data, and Bunny requires real citations, reference paper IDs, or DOIs to curate a dataset from the citation/reference network, this example serves only as a demo of how to interact with Bunny.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows async co-routines to work inside of jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   eid               5 non-null      object \n",
      " 1   s2id              5 non-null      object \n",
      " 2   doi               5 non-null      object \n",
      " 3   title             5 non-null      object \n",
      " 4   abstract          5 non-null      object \n",
      " 5   year              5 non-null      int64  \n",
      " 6   authors           5 non-null      object \n",
      " 7   author_ids        5 non-null      object \n",
      " 8   affiliations      5 non-null      object \n",
      " 9   funding           0 non-null      object \n",
      " 10  PACs              1 non-null      object \n",
      " 11  publication_name  5 non-null      object \n",
      " 12  subject_areas     5 non-null      object \n",
      " 13  s2_authors        5 non-null      object \n",
      " 14  s2_author_ids     5 non-null      object \n",
      " 15  citations         5 non-null      object \n",
      " 16  references        4 non-null      object \n",
      " 17  num_citations     5 non-null      int64  \n",
      " 18  num_references    5 non-null      float64\n",
      "dtypes: float64(1), int64(2), object(16)\n",
      "memory usage: 892.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(os.path.join(\"..\", \"..\", \"data\", \"sample2.csv\")).head(5)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup AutoBunny Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoBunny uses a list of steps to take. Each step (wrapped in an ```AutoBunnyStep``` class consists of three arguments:\n",
    "\n",
    "    1. Hop Mode(s): modes, list of str, str\n",
    "         Which features to use for expansion of the dataset (currently 'citations', 'references',\n",
    "         's2_author_ids')\n",
    "    2. Max Papers: max_papers, int\n",
    "         The upper bound on how many papers to return for a given hop. If not set, as many papers as possible\n",
    "         are returned.\n",
    "    3. Hop Priority: hop_priority, str\n",
    "          How papers should be prioritized in the lookup if max_papers is defined. The options are `random` and \n",
    "          `frequency`. The `random` option shuffles the items prior to search. The `frequency` option looks for \n",
    "          the most common items first. \n",
    "    4. Cheetah Settings:  cheetah_settings, dict\n",
    "         Which settings to use for filtering the search results\n",
    "    5. Vulture Setttings: vulture_settings, dict\n",
    "         Which settings to use for text cleaning. In this example this third argument is not passed \n",
    "         and left as the default implemented in the AutoBunnyStep. If a third argument is passed, \n",
    "         the default is overwritten.\n",
    "         \n",
    "For each step, the dataset will be expanded and pruned automatically using the specified settings. Early termination conditions like no papers left after filtering or maximum allowed papers in expansion apply.\n",
    "\n",
    "A single AutoBunnyStep is defined as:\n",
    "\n",
    "```python\n",
    "AutoBunnyStep(\n",
    "    modes: list\n",
    "    max_papers: int = 0\n",
    "    hop_priority: str = 'random'  # 'random', 'frequency`\n",
    "    cheetah_settings: dict = field(default_factory = lambda: {'query': None})\n",
    "    vulture_settings: dict = field(default_factory = lambda: [])\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.applications.Bunny import AutoBunnyStep\n",
    "\n",
    "cheetah_settings = {\n",
    "    \"query\": \"tensor\",\n",
    "    \"in_title\":False, \n",
    "    \"in_abstract\":True,\n",
    "}\n",
    "\n",
    "steps = [\n",
    "    AutoBunnyStep(['references'], cheetah_settings=cheetah_settings),\n",
    "    AutoBunnyStep(['citations'], cheetah_settings=cheetah_settings),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use Bunny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found S2_KEY environment variable\n"
     ]
    }
   ],
   "source": [
    "if \"S2_KEY\" in os.environ:\n",
    "    print(\"Found S2_KEY environment variable\")\n",
    "    API_KEY = os.environ[\"S2_KEY\"]\n",
    "else:\n",
    "    print(\"Variable does not exist. Export SemanticScholar API key on your environment using the variable name S2_KEY.\")\n",
    "    API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.applications.Bunny import AutoBunny\n",
    "\n",
    "ab = AutoBunny(df, s2_key=API_KEY, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Vulture]: Cleaning 5 documents\n",
      "  0%|          | 0/1 [00:00<?, ?it/s][Vulture]: Running SimpleCleaner module\n",
      "[Parallel(n_jobs=5)]: Using backend MultiprocessingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.3s remaining:    0.5s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.4s finished\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.42it/s]\n",
      "/home/barron/miniconda3/envs/dev_artic_fox/lib/python3.11/site-packages/TELF/applications/Cheetah/cheetah.py:1093: RuntimeWarning: 'None' not found in DataFrame. Removing the title index\n",
      "  warnings.warn(f\"'{columns[d]}' not found in DataFrame. Removing the {d} index\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing index.\n",
      "Indexing abstract\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 5786.84it/s]\n",
      "100%|██████████| 398/398 [00:00<00:00, 3551772.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing years\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 127875.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing author IDs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 135300.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 33825.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing affiliations and countries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 26149.03it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 107546.26it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 112347.43it/s]\n",
      "Found 3 papers in 0.0011 seconds\n",
      "[Bunny]: Downloading papers for hop 1\n",
      "  0%|          | 0/18 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: hop_df is None for i = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[S2]: Finished downloading 0 papers for given query in 0.99s\n",
      "[Vulture]: Cleaning 5 documents\n",
      "  0%|          | 0/1 [00:00<?, ?it/s][Vulture]: Running SimpleCleaner module\n",
      "[Parallel(n_jobs=5)]: Using backend MultiprocessingBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   2 out of   5 | elapsed:    0.4s remaining:    0.5s\n",
      "[Parallel(n_jobs=5)]: Done   5 out of   5 | elapsed:    0.4s finished\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.31it/s]\n",
      "/home/barron/miniconda3/envs/dev_artic_fox/lib/python3.11/site-packages/TELF/applications/Cheetah/cheetah.py:1093: RuntimeWarning: 'None' not found in DataFrame. Removing the title index\n",
      "  warnings.warn(f\"'{columns[d]}' not found in DataFrame. Removing the {d} index\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing index.\n",
      "Indexing abstract\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 6636.56it/s]\n",
      "100%|██████████| 398/398 [00:00<00:00, 1570397.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing years\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 59074.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing author IDs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 84222.97it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 20560.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing affiliations and countries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 13609.03it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 62291.64it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 64527.75it/s]\n",
      "Found 3 papers in 0.0021 seconds\n",
      "[Bunny]: Downloading papers for hop 1\n",
      "  0%|          | 0/70 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: hop_df is None for i = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[S2]: Finished downloading 0 papers for given query in 1.64s\n",
      "[Vulture]: Cleaning 3 documents\n",
      "  0%|          | 0/1 [00:00<?, ?it/s][Vulture]: Running SimpleCleaner module\n",
      "[Parallel(n_jobs=3)]: Using backend MultiprocessingBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   3 out of   3 | elapsed:    0.3s finished\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "/home/barron/miniconda3/envs/dev_artic_fox/lib/python3.11/site-packages/TELF/applications/Cheetah/cheetah.py:1093: RuntimeWarning: 'None' not found in DataFrame. Removing the title index\n",
      "  warnings.warn(f\"'{columns[d]}' not found in DataFrame. Removing the {d} index\", RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing index.\n",
      "Indexing abstract\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 2573.72it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 1446311.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing years\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 22753.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing author IDs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 42945.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 21183.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing affiliations and countries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 13603.15it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 59918.63it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 67288.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 20 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   eid               3 non-null      object \n",
      " 1   s2id              3 non-null      object \n",
      " 2   doi               3 non-null      object \n",
      " 3   title             3 non-null      object \n",
      " 4   abstract          3 non-null      object \n",
      " 5   year              3 non-null      int64  \n",
      " 6   authors           3 non-null      object \n",
      " 7   author_ids        3 non-null      object \n",
      " 8   affiliations      3 non-null      object \n",
      " 9   funding           0 non-null      object \n",
      " 10  PACs              0 non-null      object \n",
      " 11  publication_name  3 non-null      object \n",
      " 12  subject_areas     3 non-null      object \n",
      " 13  s2_authors        3 non-null      object \n",
      " 14  s2_author_ids     3 non-null      object \n",
      " 15  citations         3 non-null      object \n",
      " 16  references        2 non-null      object \n",
      " 17  num_citations     3 non-null      int64  \n",
      " 18  num_references    3 non-null      float64\n",
      " 19  type              3 non-null      int64  \n",
      "dtypes: float64(1), int64(3), object(16)\n",
      "memory usage: 612.0+ bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found 3 papers in 0.0016 seconds\n"
     ]
    }
   ],
   "source": [
    "ab_df = ab.run(steps)\n",
    "ab_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_artic_fox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
