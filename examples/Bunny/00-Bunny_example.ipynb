{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note that this notebook is a demonstration of how to use Bunny. Since we are using mock data, and Bunny requires real citations, reference paper IDs, or DOIs to curate a dataset from the citation/reference network, this example serves only as a demo of how to interact with Bunny.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows async co-routines to work inside of jupyter notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 19 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   eid               5 non-null      object \n",
      " 1   s2id              5 non-null      object \n",
      " 2   doi               5 non-null      object \n",
      " 3   title             5 non-null      object \n",
      " 4   abstract          5 non-null      object \n",
      " 5   year              5 non-null      int64  \n",
      " 6   authors           5 non-null      object \n",
      " 7   author_ids        5 non-null      object \n",
      " 8   affiliations      5 non-null      object \n",
      " 9   funding           0 non-null      object \n",
      " 10  PACs              1 non-null      object \n",
      " 11  publication_name  5 non-null      object \n",
      " 12  subject_areas     5 non-null      object \n",
      " 13  s2_authors        5 non-null      object \n",
      " 14  s2_author_ids     5 non-null      object \n",
      " 15  citations         5 non-null      object \n",
      " 16  references        4 non-null      object \n",
      " 17  num_citations     5 non-null      int64  \n",
      " 18  num_references    5 non-null      float64\n",
      "dtypes: float64(1), int64(2), object(16)\n",
      "memory usage: 892.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(os.path.join(\"..\", \"..\", \"data\", \"sample2.csv\")).head(5)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup AutoBunny Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoBunny uses a list of steps to take. Each step (wrapped in an ```AutoBunnyStep``` class consists of three arguments:\n",
    "\n",
    "    1. Hop Mode(s): modes, list of str, str\n",
    "         Which features to use for expansion of the dataset (currently 'citations', 'references',\n",
    "         's2_author_ids')\n",
    "    2. Max Papers: max_papers, int\n",
    "         The upper bound on how many papers to return for a given hop. If not set, as many papers as possible\n",
    "         are returned.\n",
    "    3. Hop Priority: hop_priority, str\n",
    "          How papers should be prioritized in the lookup if max_papers is defined. The options are `random` and \n",
    "          `frequency`. The `random` option shuffles the items prior to search. The `frequency` option looks for \n",
    "          the most common items first. \n",
    "    4. Cheetah Settings:  cheetah_settings, dict\n",
    "         Which settings to use for filtering the search results\n",
    "    5. Vulture Setttings: vulture_settings, dict\n",
    "         Which settings to use for text cleaning. In this example this third argument is not passed \n",
    "         and left as the default implemented in the AutoBunnyStep. If a third argument is passed, \n",
    "         the default is overwritten.\n",
    "         \n",
    "For each step, the dataset will be expanded and pruned automatically using the specified settings. Early termination conditions like no papers left after filtering or maximum allowed papers in expansion apply.\n",
    "\n",
    "A single AutoBunnyStep is defined as:\n",
    "\n",
    "```python\n",
    "AutoBunnyStep(\n",
    "    modes: list\n",
    "    max_papers: int = 0\n",
    "    hop_priority: str = 'random'  # 'random', 'frequency`\n",
    "    cheetah_settings: dict = field(default_factory = lambda: {'query': None})\n",
    "    vulture_settings: dict = field(default_factory = lambda: [])\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.applications.Bunny import AutoBunnyStep\n",
    "\n",
    "cheetah_settings = {\n",
    "    \"query\": \"tensor\",\n",
    "    \"in_title\":False, \n",
    "    \"in_abstract\":True,\n",
    "}\n",
    "\n",
    "steps = [\n",
    "    AutoBunnyStep(['references'], cheetah_settings=cheetah_settings),\n",
    "    AutoBunnyStep(['citations'], cheetah_settings=cheetah_settings),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use Bunny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found S2_KEY environment variable\n"
     ]
    }
   ],
   "source": [
    "if \"S2_KEY\" in os.environ:\n",
    "    print(\"Found S2_KEY environment variable\")\n",
    "    API_KEY = os.environ[\"S2_KEY\"]\n",
    "else:\n",
    "    print(\"Variable does not exist. Export SemanticScholar API key on your environment using the variable name S2_KEY.\")\n",
    "    API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TELF.applications.Bunny import AutoBunny\n",
    "\n",
    "ab = AutoBunny(df, s2_key=API_KEY, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_df = ab.run(steps)\n",
    "ab_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TELF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
